{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mryab/efficient-dl-systems/blob/main/week04_large_models/practice_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlQVQby9YJip"
   },
   "source": [
    "### Efficient DL Practice: Advanced Parallelism (5 points)\n",
    "\n",
    "In this practice session, we'll cover techniques for training large models in parallel: **Model** and **Sequence Parallelism**.\n",
    "More precisely, you will implement them, and we will root for you as you go. Good luck, ðŸ¥©ðŸ‘œ!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cp865qOmgcl",
    "outputId": "02202d9e-12d4-4341-b31f-006c356bd2b7"
   },
   "outputs": [],
   "source": [
    "# dependencies: the code will likely work with slightly newer/older versions, but may require minimal patching\n",
    "# %pip install -q transformers==4.48.3 peft==0.14.0\n",
    "\n",
    "import transformers; assert transformers.__version__.startswith(\"4.48\"), transformers.__version__\n",
    "import peft; assert peft.__version__.startswith(\"0.14\"), peft.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tw_-8huWabX1"
   },
   "source": [
    "__Part 1: Tensor Parallelism (2 points)__\n",
    "![img](https://pytorch.org/tutorials/_images/megatron_lm.png)\n",
    "\n",
    "We'll begin by implementing a simple tensor parallelism (also known as the [original](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) model parallelism).\n",
    "\n",
    "Our ultimate objective is to run and fine-tune a Llama 3.x model in tensor-parallel mode. However, it is rather difficult to do that in one go, especially if you take bugs into account. So we'll start simple: __here's a single Llama MLP module:__\n",
    "\n",
    "`please read the code below carefully, it's a template for the remaining assgnments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbcTjrsRB-ps",
    "outputId": "48576f02-d041-42be-dc44-104e8d84251a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tensor_parallel_mlp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tensor_parallel_mlp.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):  #  based on llama 3.1 8B configuration\n",
    "    def __init__(self, hidden_size: int = 4096, intermediate_size: int = 14336):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.down_proj(F.silu(self.gate_proj(input)) * self.up_proj(input))\n",
    "\n",
    "\n",
    "class ComputeWithAllReduce(torch.autograd.Function):\n",
    "    @staticmethod  # fun fact: torch.distributed.nn has differentiable all_reduce!\n",
    "    def forward(ctx, tp_shard: nn.Module, input: torch.Tensor):\n",
    "        input = input.detach().requires_grad_(input.requires_grad)\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx._tp_shard = tp_shard\n",
    "        output = tp_shard(input)\n",
    "        dist.all_reduce(output)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        with torch.enable_grad():\n",
    "          output = ctx._tp_shard(ctx.saved_tensors[0])\n",
    "          output.backward(grad_output)\n",
    "        dist.all_reduce(ctx.saved_tensors[0].grad)\n",
    "        return None, ctx.saved_tensors[0].grad\n",
    "\n",
    "\n",
    "class AllReduceModule(nn.Sequential):\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        return ComputeWithAllReduce.apply(super().forward, input)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dist.init_process_group(\"gloo\")   # use nccl for cuda devices\n",
    "    torch.manual_seed(1337)           # init weights equally on all ranks\n",
    "    rank, world_size = dist.get_rank(), dist.get_world_size()\n",
    "\n",
    "    for active_rank in range(world_size):\n",
    "      dist.barrier()  # initialize each rank sequentially to save system RAM\n",
    "      if rank != active_rank: continue\n",
    "\n",
    "      # we will now implement Tensor Parallelism for the ref_module below:\n",
    "      ref_module = nn.Sequential(nn.RMSNorm(4096), LlamaMLP())\n",
    "      # compute reference tensors to test against them later\n",
    "      input = torch.randn(1, 4096, requires_grad=True)\n",
    "      ref_output = ref_module(input)\n",
    "      ref_output.sum().backward()\n",
    "      ref_input_grad = input.grad.clone()\n",
    "\n",
    "      # TP step 1: define a module that computes a portion of intermediate units\n",
    "      intermediate_size = ref_module[1].down_proj.in_features\n",
    "      local_units = intermediate_size // world_size\n",
    "      assert intermediate_size % world_size == 0\n",
    "      tp_module = nn.Sequential(   # assign a portion of units per rank --v\n",
    "          nn.RMSNorm(4096), AllReduceModule(LlamaMLP(intermediate_size=local_units))\n",
    "      )   # all-reduce outputs during forward, all-reduce gradients on backward\n",
    "\n",
    "      with torch.no_grad():  # copy select weights from the reference MLP\n",
    "        # v-- input norm layer is too small to bother parallelizing - we replicate it!\n",
    "        tp_module[0].load_state_dict(ref_module[0].state_dict())\n",
    "        # up and gate projections are sharded across output units\n",
    "        unit_slice = slice(local_units * rank, local_units * (rank + 1))\n",
    "        tp_module[1][0].up_proj.weight[...] = ref_module[1].up_proj.weight[unit_slice]\n",
    "        tp_module[1][0].gate_proj.weight[...] = ref_module[1].gate_proj.weight[unit_slice]\n",
    "        # down projection is sharded across input units, matching up/gate proj\n",
    "        tp_module[1][0].down_proj.weight[...] = ref_module[1].down_proj.weight[:, unit_slice]\n",
    "      print(f\"Initialized {rank=}\", flush=True)\n",
    "      del ref_module  # free RAM for next rank\n",
    "\n",
    "    dist.barrier()  # test 1: forward pass\n",
    "    tp_input = input.detach().requires_grad_(True)\n",
    "    tp_output = tp_module(tp_input)\n",
    "    if rank == 0:\n",
    "        print(f\"\\nReference outputs ({rank=}):\", ref_output.data, flush=True)\n",
    "    for i in range(world_size):\n",
    "        dist.barrier()\n",
    "        if i != rank: continue\n",
    "        print(f\"TParallel outputs ({rank=}):\", tp_output.data, flush=True)\n",
    "        assert torch.allclose(tp_output, ref_output, atol=1e-4), f\"output mismatch on {rank=}\"\n",
    "\n",
    "    dist.barrier()  # test 2: backward w.r.t. inputs\n",
    "    assert tp_input.grad is None\n",
    "    tp_output.sum().backward()\n",
    "    if rank == 0:\n",
    "        print(f\"\\nReference input grad ({rank=}):\", ref_input_grad, flush=True)\n",
    "    for i in range(world_size):\n",
    "        dist.barrier()\n",
    "        if i != rank: continue\n",
    "        print(f\"TParallel input grad ({rank=}):\", tp_input.grad.data, flush=True)\n",
    "        assert torch.allclose(tp_input.grad, ref_input_grad, atol=1e-4), f\"input_grad mismatch on {rank=}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4z99fF4HC9hp",
    "outputId": "e4d08391-746c-47a5-f137-c4b1e56b64c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "Initialized rank=0\n",
      "Initialized rank=1\n",
      "\n",
      "Reference outputs (rank=0): tensor([[-0.1145,  0.0160,  0.0500,  ..., -0.1455,  0.1126, -0.0192]])\n",
      "TParallel outputs (rank=0): tensor([[-0.1145,  0.0160,  0.0500,  ..., -0.1455,  0.1126, -0.0192]])\n",
      "TParallel outputs (rank=1): tensor([[-0.1145,  0.0160,  0.0500,  ..., -0.1455,  0.1126, -0.0192]])\n",
      "\n",
      "Reference input grad (rank=0): tensor([[ 0.0343, -0.2492, -0.1858,  ..., -0.0541,  0.0388, -0.1529]])\n",
      "TParallel input grad (rank=0): tensor([[ 0.0343, -0.2492, -0.1858,  ..., -0.0541,  0.0388, -0.1529]])\n",
      "TParallel input grad (rank=1): tensor([[ 0.0343, -0.2492, -0.1858,  ..., -0.0541,  0.0388, -0.1529]])\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=1 torchrun --nproc_per_node 2 tensor_parallel_mlp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMnNI9I5d1R_"
   },
   "source": [
    "Note that the code above lacks two details:\n",
    "- it uses a form of checkpointing, but does not save random state, which would be required if you use dropout;\n",
    "- it replicates RMSNorm, but it is not synchronized. Training would require all-reduce-ing gradients for those layers, e.g. by wrapping them with DDP.\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__Task 1 (1 point):__ Implement tensor-parallel multi-head attention.\n",
    "\n",
    "Like with the MLP module before, you can partition attention across multiple devices. This time, every device is to compute a portion of whole attention **heads** (and not individual units). We exploit the property that an multi-head attention layer can be viewed as a sum of individual head outputs after output projection.\n",
    "\n",
    "For the sake of formality, this is the computation you need to parallelize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ub-VLG38kpsl",
    "outputId": "e488fc79-0168-4d42-dd41-8f7892de2958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output=tensor([[[ 0.0154,  0.0549,  0.0559,  ...,  0.0129,  0.0206,  0.0052],\n",
      "         [ 0.0193,  0.0483,  0.0460,  ...,  0.0106,  0.0001, -0.0131],\n",
      "         [ 0.0106,  0.0493,  0.0357,  ...,  0.0233, -0.0114,  0.0006],\n",
      "         ...,\n",
      "         [ 0.0092,  0.0660,  0.0353,  ...,  0.0057, -0.0149,  0.0451],\n",
      "         [ 0.0254,  0.0371,  0.0443,  ...,  0.0201,  0.0063,  0.0100],\n",
      "         [ 0.0117,  0.0552,  0.0449,  ...,  0.0130, -0.0110, -0.0079]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "input.grad=tensor([[[-0.0015, -0.0002,  0.0002,  ...,  0.0024, -0.0006,  0.0011],\n",
      "         [-0.0016, -0.0003,  0.0003,  ...,  0.0025, -0.0007,  0.0009],\n",
      "         [-0.0016, -0.0003,  0.0003,  ...,  0.0026, -0.0007,  0.0010],\n",
      "         ...,\n",
      "         [-0.0016, -0.0003,  0.0002,  ...,  0.0024, -0.0007,  0.0009],\n",
      "         [-0.0016, -0.0002,  0.0004,  ...,  0.0026, -0.0007,  0.0009],\n",
      "         [-0.0016, -0.0002,  0.0002,  ...,  0.0025, -0.0006,  0.0011]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.models.llama.modeling_llama import LlamaConfig, LlamaAttention, LlamaRotaryEmbedding\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B\"  # for testing (but not grading!), you may want to use Maykeye/TinyLLama-v0\n",
    "config = LlamaConfig.from_pretrained(MODEL_NAME)\n",
    "layer = LlamaAttention(config, layer_idx=5)\n",
    "rotary_emb = LlamaRotaryEmbedding(config)\n",
    "\n",
    "input = torch.randn(1, 128, config.hidden_size, requires_grad=True)\n",
    "position_embeddings = rotary_emb(input, position_ids=torch.arange(128)[None])\n",
    "\n",
    "output, *_etc = layer(input, attention_mask=None, position_embeddings=position_embeddings)\n",
    "print(f\"{output=}\")\n",
    "output.norm().backward()\n",
    "print(f\"{input.grad=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRYcD0qxmTuQ"
   },
   "source": [
    "Same as before, your task is to create a multi-head attention layer, partition it across ranks and verify two things:\n",
    "- attention outputs on the same inputs (and mask) match with the non-parallel version;\n",
    "- gradients w.r.t. attention inputs are the same; gradients w.r.t. mask need not be verified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128001,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 16,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pad_token_id\": 128004,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.48.3\",\n",
       "  \"unsloth_fixed\": true,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6SrcK3avd3GA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tensor_parallel_attn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tensor_parallel_attn.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LlamaConfig, LlamaAttention, LlamaRotaryEmbedding\n",
    "from torch import nn\n",
    "import torch\n",
    "from typing import Dict, Callable, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B\"  # for testing (but not grading!), you may want to use Maykeye/TinyLLama-v0\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def sdpa_attention_forward(\n",
    "    module: torch.nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    dropout: float = 0.0,\n",
    "    scaling: Optional[float] = None,\n",
    "    is_causal: Optional[bool] = None,\n",
    "    **kwargs,\n",
    ") -> Tuple[torch.Tensor, None]:\n",
    "    if hasattr(module, \"num_key_value_groups\"):\n",
    "        key = repeat_kv(key, module.num_key_value_groups)\n",
    "        value = repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    causal_mask = attention_mask\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n",
    "\n",
    "    query = query.contiguous()\n",
    "    key = key.contiguous()\n",
    "    value = value.contiguous()\n",
    "\n",
    "    if is_causal is None:\n",
    "        is_causal = causal_mask is None and query.shape[2] > 1\n",
    "\n",
    "    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):\n",
    "        is_causal = is_causal.item()\n",
    "\n",
    "    # print('query', query.shape)\n",
    "    # print('key', key.shape)\n",
    "    # print('value', value.shape)\n",
    "    \n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=dropout,\n",
    "        scale=scaling,\n",
    "        is_causal=is_causal,\n",
    "    )\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, None\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class MyLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # print('query_states', query_states.shape)\n",
    "        # print('key_states', key_states.shape)\n",
    "        # print('value_states', value_states.shape)\n",
    "        \n",
    "        attn_output, attn_weights = sdpa_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "        )\n",
    "\n",
    "        # print('attn_output', attn_output.shape)\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class ComputeWithAllReduce(torch.autograd.Function):\n",
    "    @staticmethod  # fun fact: torch.distributed.nn has differentiable all_reduce!\n",
    "    def forward(ctx, tp_shard: nn.Module, input: torch.Tensor, **kwargs):\n",
    "        input = input.detach().requires_grad_(input.requires_grad)\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx._tp_shard = tp_shard\n",
    "        output = tp_shard(input, **kwargs)\n",
    "        dist.all_reduce(output)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        with torch.enable_grad():\n",
    "          output = ctx._tp_shard(ctx.saved_tensors[0])\n",
    "          output.backward(grad_output)\n",
    "        dist.all_reduce(ctx.saved_tensors[0].grad)\n",
    "        return None, ctx.saved_tensors[0].grad\n",
    "\n",
    "\n",
    "# class AllReduceModule(nn.Sequential):\n",
    "#     def forward(self, input: torch.Tensor, **kwargs):\n",
    "#         fn = lambda x: super(AllReduceModule, self).forward(x, **kwargs)\n",
    "#         return ComputeWithAllReduce.apply(fn, input)\n",
    "\n",
    "class ReduceWrapper(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        fn = lambda x: self.module(x, **kwargs)\n",
    "        return ComputeWithAllReduce.apply(fn, input)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dist.init_process_group(\"nccl\")   # use nccl for cuda devices\n",
    "    torch.manual_seed(1337)           # init weights equally on all ranks\n",
    "    rank, world_size = dist.get_rank(), dist.get_world_size()\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    for active_rank in range(world_size):\n",
    "      dist.barrier()  # initialize each rank sequentially to save system RAM\n",
    "      if rank != active_rank: continue\n",
    "\n",
    "      # we will now implement Tensor Parallelism for the ref_module below:\n",
    "      config = LlamaConfig.from_pretrained(MODEL_NAME)\n",
    "      ref_module = MyLlamaAttention(config, layer_idx=5).cuda()\n",
    "      rotary_emb = LlamaRotaryEmbedding(config).cuda()\n",
    "      # ^-- you may need to modify this code, e.g. pass parameters or use transformers LlamaAttention (as above)\n",
    "\n",
    "      # generate reference tensors to test against them later\n",
    "      input = torch.randn(1, 128, 2048, requires_grad=True, device='cuda')\n",
    "      position_embeddings = rotary_emb(input, position_ids=torch.arange(128)[None].cuda())\n",
    "      extra_inputs = dict(attention_mask=None, position_embeddings=position_embeddings)  # <-- OPTIONAL: either design additional inputs here, as in the reference above\n",
    "\n",
    "      ref_output = ref_module(input, **extra_inputs).cuda()\n",
    "      ref_output.sum().backward()\n",
    "      ref_input_grad = input.grad.clone()\n",
    "\n",
    "      # TP step 1: define a module that computes a portion of attention heads\n",
    "\n",
    "      smaller_config = deepcopy(config)\n",
    "      assert config.num_attention_heads % world_size == 0\n",
    "      smaller_config.num_attention_heads = config.num_attention_heads // world_size\n",
    "      smaller_config.num_key_value_heads = config.num_key_value_heads // world_size\n",
    "      tp_module = ReduceWrapper(MyLlamaAttention(smaller_config, layer_idx=5)).cuda()  # create a tensor-parallel version of the Attention module\n",
    "\n",
    "      with torch.no_grad():\n",
    "        #   print('hidden_size', config.hidden_size)\n",
    "        #   print('num_key_value_heads', smaller_config.num_key_value_heads)\n",
    "        #   print('num_attention_heads', smaller_config.num_attention_heads)\n",
    "        #   print('orig num_key_value_heads', config.num_key_value_heads)\n",
    "        #   print('orig num_attention_heads', config.num_attention_heads)\n",
    "        #   print('ref_module.k_proj.weight.data', ref_module.k_proj.weight.data.shape)\n",
    "          local_units = config.head_dim * smaller_config.num_key_value_heads\n",
    "          unit_slice = slice(local_units * rank, local_units * (rank + 1))\n",
    "          tp_module.module.k_proj.weight.data = ref_module.k_proj.weight.data[unit_slice]\n",
    "          tp_module.module.v_proj.weight.data = ref_module.v_proj.weight.data[unit_slice]\n",
    "          local_units = config.head_dim * smaller_config.num_attention_heads\n",
    "          unit_slice = slice(local_units * rank, local_units * (rank + 1))\n",
    "        #   print('tp_module[0].o_proj.weight.data', tp_module[0].o_proj.weight.data.shape)\n",
    "        #   print('ref_module.o_proj.weight.data', ref_module.o_proj.weight.data.shape)\n",
    "          tp_module.module.o_proj.weight.data[...] = ref_module.o_proj.weight.data[:, unit_slice]\n",
    "          tp_module.module.q_proj.weight.data[...] = ref_module.q_proj.weight.data[unit_slice]\n",
    "\n",
    "      print(f\"Initialized {rank=}\", flush=True)\n",
    "      del ref_module  # free RAM for next rank\n",
    "\n",
    "    # TEST AREA: you are free to add additional parameters, but your code *must* run the same tests as below\n",
    "    dist.barrier()  # test 1: forward pass\n",
    "    tp_input = input.detach().cuda().requires_grad_(True)\n",
    "    tp_output = tp_module(tp_input, **extra_inputs).cuda()\n",
    "    print(ref_output.data)\n",
    "    if rank == 0:\n",
    "        print(f\"\\nReference outputs ({rank=}):\", ref_output.data, flush=True)\n",
    "    for i in range(world_size):\n",
    "        dist.barrier()\n",
    "        if i != rank: continue\n",
    "        print(f\"TParallel outputs ({rank=}):\", tp_output.data, flush=True)\n",
    "        assert torch.allclose(tp_output, ref_output, atol=1e-5), f\"output mismatch on {rank=}\"\n",
    "\n",
    "    dist.barrier()  # test 2: backward w.r.t. inputs\n",
    "    assert tp_input.grad is None\n",
    "    tp_output.sum().backward()\n",
    "    if rank == 0:\n",
    "        print(f\"\\nReference input grad ({rank=}):\", ref_input_grad, flush=True)\n",
    "    for i in range(world_size):\n",
    "        dist.barrier()\n",
    "        if i != rank: continue\n",
    "        print(f\"TParallel input grad ({rank=}):\", tp_input.grad.data, flush=True)\n",
    "        assert torch.allclose(tp_input.grad, ref_input_grad, atol=1e-4), f\"input_grad mismatch on {rank=}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "yodlDa_cnPWH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  return func(*args, **kwargs)\n",
      "[rank0]:[W223 08:15:40.920344842 ProcessGroupNCCL.cpp:5138] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()\n",
      "Initialized rank=0\n",
      "Initialized rank=1\n",
      "tensor([[[ 2.2773e-01,  7.7059e-01, -1.7579e-02,  ...,  3.0142e-02,\n",
      "           2.6831e-01,  2.4405e-01],\n",
      "         [ 1.7466e-02,  5.4525e-03, -1.2642e-01,  ..., -4.0684e-02,\n",
      "           5.9789e-02, -3.3783e-01],\n",
      "         [-2.4119e-03,  1.2824e-01,  8.7102e-02,  ...,  1.6749e-01,\n",
      "           4.9222e-02, -1.8924e-02],\n",
      "         ...,\n",
      "         [ 4.8462e-02,  6.5924e-02,  1.1517e-03,  ...,  1.2480e-02,\n",
      "           2.9387e-02,  5.1865e-03],\n",
      "         [ 5.5688e-02,  5.9044e-02,  2.2112e-02,  ..., -1.8325e-04,\n",
      "           2.2680e-02, -4.4655e-04],\n",
      "         [ 4.1614e-02,  7.1609e-02,  2.4544e-02,  ...,  2.0214e-02,\n",
      "           1.6948e-02,  1.6181e-02]]], device='cuda:1')\n",
      "tensor([[[ 2.2773e-01,  7.7059e-01, -1.7579e-02,  ...,  3.0142e-02,\n",
      "           2.6831e-01,  2.4405e-01],\n",
      "         [ 1.7466e-02,  5.4525e-03, -1.2642e-01,  ..., -4.0684e-02,\n",
      "           5.9789e-02, -3.3783e-01],\n",
      "         [-2.4119e-03,  1.2824e-01,  8.7102e-02,  ...,  1.6749e-01,\n",
      "           4.9222e-02, -1.8924e-02],\n",
      "         ...,\n",
      "         [ 4.8462e-02,  6.5924e-02,  1.1517e-03,  ...,  1.2480e-02,\n",
      "           2.9387e-02,  5.1865e-03],\n",
      "         [ 5.5688e-02,  5.9044e-02,  2.2112e-02,  ..., -1.8325e-04,\n",
      "           2.2680e-02, -4.4655e-04],\n",
      "         [ 4.1614e-02,  7.1609e-02,  2.4544e-02,  ...,  2.0214e-02,\n",
      "           1.6948e-02,  1.6181e-02]]], device='cuda:0')\n",
      "\n",
      "Reference outputs (rank=0): tensor([[[ 2.2773e-01,  7.7059e-01, -1.7579e-02,  ...,  3.0142e-02,\n",
      "           2.6831e-01,  2.4405e-01],\n",
      "         [ 1.7466e-02,  5.4525e-03, -1.2642e-01,  ..., -4.0684e-02,\n",
      "           5.9789e-02, -3.3783e-01],\n",
      "         [-2.4119e-03,  1.2824e-01,  8.7102e-02,  ...,  1.6749e-01,\n",
      "           4.9222e-02, -1.8924e-02],\n",
      "         ...,\n",
      "         [ 4.8462e-02,  6.5924e-02,  1.1517e-03,  ...,  1.2480e-02,\n",
      "           2.9387e-02,  5.1865e-03],\n",
      "         [ 5.5688e-02,  5.9044e-02,  2.2112e-02,  ..., -1.8325e-04,\n",
      "           2.2680e-02, -4.4655e-04],\n",
      "         [ 4.1614e-02,  7.1609e-02,  2.4544e-02,  ...,  2.0214e-02,\n",
      "           1.6948e-02,  1.6181e-02]]], device='cuda:0')\n",
      "TParallel outputs (rank=0): tensor([[[ 2.2773e-01,  7.7059e-01, -1.7579e-02,  ...,  3.0142e-02,\n",
      "           2.6831e-01,  2.4405e-01],\n",
      "         [ 1.7466e-02,  5.4525e-03, -1.2642e-01,  ..., -4.0684e-02,\n",
      "           5.9789e-02, -3.3783e-01],\n",
      "         [-2.4120e-03,  1.2824e-01,  8.7102e-02,  ...,  1.6749e-01,\n",
      "           4.9222e-02, -1.8924e-02],\n",
      "         ...,\n",
      "         [ 4.8462e-02,  6.5924e-02,  1.1517e-03,  ...,  1.2479e-02,\n",
      "           2.9387e-02,  5.1864e-03],\n",
      "         [ 5.5688e-02,  5.9044e-02,  2.2112e-02,  ..., -1.8326e-04,\n",
      "           2.2680e-02, -4.4655e-04],\n",
      "         [ 4.1614e-02,  7.1609e-02,  2.4544e-02,  ...,  2.0214e-02,\n",
      "           1.6948e-02,  1.6181e-02]]], device='cuda:0')\n",
      "TParallel outputs (rank=1): tensor([[[ 2.2773e-01,  7.7059e-01, -1.7579e-02,  ...,  3.0142e-02,\n",
      "           2.6831e-01,  2.4405e-01],\n",
      "         [ 1.7466e-02,  5.4525e-03, -1.2642e-01,  ..., -4.0684e-02,\n",
      "           5.9789e-02, -3.3783e-01],\n",
      "         [-2.4120e-03,  1.2824e-01,  8.7102e-02,  ...,  1.6749e-01,\n",
      "           4.9222e-02, -1.8924e-02],\n",
      "         ...,\n",
      "         [ 4.8462e-02,  6.5924e-02,  1.1517e-03,  ...,  1.2479e-02,\n",
      "           2.9387e-02,  5.1864e-03],\n",
      "         [ 5.5688e-02,  5.9044e-02,  2.2112e-02,  ..., -1.8326e-04,\n",
      "           2.2680e-02, -4.4655e-04],\n",
      "         [ 4.1614e-02,  7.1609e-02,  2.4544e-02,  ...,  2.0214e-02,\n",
      "           1.6948e-02,  1.6181e-02]]], device='cuda:1')\n",
      "\n",
      "Reference input grad (rank=0): tensor([[[-1.6699e+00,  1.3298e+00,  8.2987e-01,  ..., -8.3731e-01,\n",
      "          -1.0598e-01, -8.6792e-01],\n",
      "         [-1.5316e+00,  9.1352e-01,  9.7734e-01,  ..., -9.2403e-01,\n",
      "          -2.3358e-02, -8.3134e-01],\n",
      "         [-1.2695e+00,  7.8515e-01,  7.3720e-01,  ..., -5.7415e-01,\n",
      "          -1.6470e-01, -6.9666e-01],\n",
      "         ...,\n",
      "         [-1.5540e-02, -6.1039e-03,  1.1323e-02,  ...,  6.5115e-04,\n",
      "          -4.2601e-03, -1.7401e-02],\n",
      "         [-1.1907e-02, -6.0330e-03,  1.2272e-02,  ...,  8.0301e-03,\n",
      "          -1.6516e-04, -9.4367e-03],\n",
      "         [-1.6083e-02, -4.1109e-03,  5.2779e-03,  ...,  3.6975e-03,\n",
      "           1.3822e-03, -1.8878e-02]]], device='cuda:0')\n",
      "TParallel input grad (rank=0): tensor([[[-1.6699e+00,  1.3298e+00,  8.2987e-01,  ..., -8.3732e-01,\n",
      "          -1.0597e-01, -8.6792e-01],\n",
      "         [-1.5316e+00,  9.1352e-01,  9.7734e-01,  ..., -9.2403e-01,\n",
      "          -2.3357e-02, -8.3134e-01],\n",
      "         [-1.2695e+00,  7.8515e-01,  7.3720e-01,  ..., -5.7415e-01,\n",
      "          -1.6470e-01, -6.9665e-01],\n",
      "         ...,\n",
      "         [-1.5540e-02, -6.1039e-03,  1.1323e-02,  ...,  6.5114e-04,\n",
      "          -4.2601e-03, -1.7401e-02],\n",
      "         [-1.1907e-02, -6.0330e-03,  1.2272e-02,  ...,  8.0301e-03,\n",
      "          -1.6515e-04, -9.4367e-03],\n",
      "         [-1.6083e-02, -4.1109e-03,  5.2779e-03,  ...,  3.6975e-03,\n",
      "           1.3822e-03, -1.8878e-02]]], device='cuda:0')\n",
      "TParallel input grad (rank=1): tensor([[[-1.6699e+00,  1.3298e+00,  8.2987e-01,  ..., -8.3732e-01,\n",
      "          -1.0597e-01, -8.6792e-01],\n",
      "         [-1.5316e+00,  9.1352e-01,  9.7734e-01,  ..., -9.2403e-01,\n",
      "          -2.3357e-02, -8.3134e-01],\n",
      "         [-1.2695e+00,  7.8515e-01,  7.3720e-01,  ..., -5.7415e-01,\n",
      "          -1.6470e-01, -6.9665e-01],\n",
      "         ...,\n",
      "         [-1.5540e-02, -6.1039e-03,  1.1323e-02,  ...,  6.5114e-04,\n",
      "          -4.2601e-03, -1.7401e-02],\n",
      "         [-1.1907e-02, -6.0330e-03,  1.2272e-02,  ...,  8.0301e-03,\n",
      "          -1.6515e-04, -9.4367e-03],\n",
      "         [-1.6083e-02, -4.1109e-03,  5.2779e-03,  ...,  3.6975e-03,\n",
      "           1.3822e-03, -1.8878e-02]]], device='cuda:1')\n",
      "[rank0]:[W223 08:15:42.445100809 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=1 torchrun --nproc_per_node 2 tensor_parallel_attn.py\n",
    "# ^-- feel free to modify parameters, as long as there are at least 2 ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxTbiVvInO9i"
   },
   "source": [
    "Well done! *(hopefully. If not, go back and, well... do it)*\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Full model conversion\n",
    "\n",
    "Now let's apply this technique to parallelize the actual Llama model. As in, with weights.\n",
    "\n",
    "__Task 2 (1 point):__ Combine the two previous techniques in one file that parallelizes an actual Llama model and .generates meaningful output. For simplicity, you do not need to partition key-value cache here - only the forward pass itself. We will default to generating tokens with recomputation.\n",
    "\n",
    "For the sake of formality, your task is to parallelize the following inference code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "152wIY8fo0TI",
    "outputId": "09394465-9b0e-4927-eb4c-343d0b61278e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01d18ca905d4174bd152927228c5655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d29b4ac66dc412d8e4bcde219e4ed0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe1bdb52a324e52b015ac6812d21a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7a6f98f8c54cffbb7d41259cb18192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722b46fc58a949bd89305b9ccbfb1aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick brown fox jumps over the lazy dog"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B\"  # for testing (but not grading!), you may want to use Maykeye/TinyLLama-v0\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)  # <-- you are allowed to switch to bf16\n",
    "\n",
    "prompt = \"A quick brown fox\"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt')[\"input_ids\"]\n",
    "print(end=prompt)\n",
    "for i in range(5):\n",
    "  with torch.no_grad():\n",
    "    new_token = model(input_ids).logits[0, -1].argmax(-1)\n",
    "    input_ids = torch.cat([input_ids, new_token.view(1, 1)], dim=1)\n",
    "  print(end=tokenizer.decode(new_token), flush=True)\n",
    "# pro tip: delete the model or restart session to free RAM for the TP experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLtaTR6zo6VU"
   },
   "source": [
    "\n",
    "**Requirements:** your code must do the following things for the full grade:\n",
    "- instantiate an actually trained Llama model (Llama 3.2 1B or larger is fine, maykeye is not)\n",
    "- run forward pass with at least 2 ranks and verify that the logits are close,\n",
    "- run backward pass w.r.t. non-parallelized input embeddings, verify that the gradients are close,\n",
    "- perform inference for 10 steps to verify that the model produces meaningful outputs (see below)\n",
    "\n",
    "You are only required to tensor-parallel-ize the transformer layers. Parallelizing embeddings and logits is optional. If you do choose to parallelize embeddings, we sincerely recommend that you partition across the embedding dim, not across tokens - so that the computation is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tensor_parallel_llama.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tensor_parallel_llama.py\n",
    "import torch\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LlamaConfig, LlamaAttention, LlamaRotaryEmbedding\n",
    "from torch import nn\n",
    "import torch\n",
    "from typing import Dict, Callable, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B\"  # for testing (but not grading!), you may want to use Maykeye/TinyLLama-v0\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def sdpa_attention_forward(\n",
    "    module: torch.nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    dropout: float = 0.0,\n",
    "    scaling: Optional[float] = None,\n",
    "    is_causal: Optional[bool] = None,\n",
    "    **kwargs,\n",
    ") -> Tuple[torch.Tensor, None]:\n",
    "    if hasattr(module, \"num_key_value_groups\"):\n",
    "        key = repeat_kv(key, module.num_key_value_groups)\n",
    "        value = repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    causal_mask = attention_mask\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n",
    "\n",
    "    query = query.contiguous()\n",
    "    key = key.contiguous()\n",
    "    value = value.contiguous()\n",
    "\n",
    "    if is_causal is None:\n",
    "        is_causal = causal_mask is None and query.shape[2] > 1\n",
    "\n",
    "    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):\n",
    "        is_causal = is_causal.item()\n",
    "\n",
    "    # print('query', query.shape)\n",
    "    # print('key', key.shape)\n",
    "    # print('value', value.shape)\n",
    "    \n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=dropout,\n",
    "        scale=scaling,\n",
    "        is_causal=is_causal,\n",
    "    )\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, None\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class MyLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor], **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # print('query_states', query_states.shape)\n",
    "        # print('key_states', key_states.shape)\n",
    "        # print('value_states', value_states.shape)\n",
    "        \n",
    "        attn_output, attn_weights = sdpa_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "        )\n",
    "\n",
    "        # print('attn_output', attn_output.shape)\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class ComputeWithAllReduce(torch.autograd.Function):\n",
    "    @staticmethod  # fun fact: torch.distributed.nn has differentiable all_reduce!\n",
    "    def forward(ctx, tp_shard: nn.Module, input: torch.Tensor, **kwargs):\n",
    "        input = input.detach().requires_grad_(input.requires_grad)\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx._tp_shard = tp_shard\n",
    "        output = tp_shard(input, **kwargs)\n",
    "        dist.all_reduce(output)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        with torch.enable_grad():\n",
    "          output = ctx._tp_shard(ctx.saved_tensors[0])\n",
    "          output.backward(grad_output)\n",
    "        dist.all_reduce(ctx.saved_tensors[0].grad)\n",
    "        return None, ctx.saved_tensors[0].grad\n",
    "\n",
    "\n",
    "class ReduceWrapper(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, hidden_states=None, **kwargs):\n",
    "        fn = lambda x: self.module(x, **kwargs)\n",
    "        output = ComputeWithAllReduce.apply(fn, hidden_states)\n",
    "        return output, None\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):  #  based on llama 3.1 8B configuration\n",
    "    def __init__(self, hidden_size: int = 4096, intermediate_size: int = 14336):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.down_proj(F.silu(self.gate_proj(input)) * self.up_proj(input))\n",
    "\n",
    "\n",
    "class ComputeWithAllReduce(torch.autograd.Function):\n",
    "    @staticmethod  # fun fact: torch.distributed.nn has differentiable all_reduce!\n",
    "    def forward(ctx, tp_shard: nn.Module, input: torch.Tensor):\n",
    "        input = input.detach().requires_grad_(input.requires_grad)\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx._tp_shard = tp_shard\n",
    "        output = tp_shard(input)\n",
    "        dist.all_reduce(output)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        with torch.enable_grad():\n",
    "          output = ctx._tp_shard(ctx.saved_tensors[0])\n",
    "          output.backward(grad_output)\n",
    "        dist.all_reduce(ctx.saved_tensors[0].grad)\n",
    "        return None, ctx.saved_tensors[0].grad\n",
    "\n",
    "\n",
    "class AllReduceModule(nn.Sequential):\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        return ComputeWithAllReduce.apply(super().forward, input)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dist.init_process_group(\"gloo\")   # use nccl for cuda devices\n",
    "    torch.manual_seed(1337)           # init weights equally on all ranks\n",
    "    rank, world_size = dist.get_rank(), dist.get_world_size()\n",
    "\n",
    "    for active_rank in range(world_size):\n",
    "        dist.barrier()\n",
    "        if rank != active_rank: continue\n",
    "        \n",
    "        MODEL_NAME = \"unsloth/Llama-3.2-1B\"  # for testing (but not grading!), you may want to use Maykeye/TinyLLama-v0\n",
    "        config = LlamaConfig.from_pretrained(MODEL_NAME)\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        ref_model = transformers.LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)  # <-- you are allowed to switch to bf16\n",
    "        prompt = \"A quick brown fox\"\n",
    "        input_ids = tokenizer(prompt, return_tensors='pt')[\"input_ids\"]\n",
    "        input = ref_model.model.embed_tokens(input_ids).detach().requires_grad_(True)\n",
    "        ref_output = ref_model(inputs_embeds=input).logits\n",
    "        ref_output.mean().backward()\n",
    "        ref_input_grad = input.grad.clone()\n",
    "        \n",
    "        tp_model = deepcopy(ref_model)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(len(tp_model.model.layers)):\n",
    "                smaller_config = deepcopy(config)\n",
    "                smaller_config.num_attention_heads = config.num_attention_heads // world_size\n",
    "                smaller_config.num_key_value_heads = config.num_key_value_heads // world_size\n",
    "                layer = ReduceWrapper(MyLlamaAttention(smaller_config, layer_idx=i))\n",
    "                local_units = config.head_dim * smaller_config.num_key_value_heads\n",
    "                unit_slice = slice(local_units * rank, local_units * (rank + 1))\n",
    "                layer.module.k_proj.weight.data = ref_model.model.layers[i].self_attn.k_proj.weight.data[unit_slice]\n",
    "                layer.module.v_proj.weight.data = ref_model.model.layers[i].self_attn.v_proj.weight.data[unit_slice]\n",
    "                local_units = config.head_dim * smaller_config.num_attention_heads\n",
    "                unit_slice = slice(local_units * rank, local_units * (rank + 1))\n",
    "                layer.module.o_proj.weight.data[...] = ref_model.model.layers[i].self_attn.o_proj.weight.data[:, unit_slice]\n",
    "                layer.module.q_proj.weight.data[...] = ref_model.model.layers[i].self_attn.q_proj.weight.data[unit_slice]\n",
    "                tp_model.model.layers[i].self_attn = layer\n",
    "\n",
    "                intermediate_size = ref_model.model.layers[i].mlp.down_proj.in_features\n",
    "                local_units = intermediate_size // world_size\n",
    "                \n",
    "                mlp_layer = AllReduceModule(LlamaMLP(hidden_size=config.hidden_size, intermediate_size=local_units))\n",
    "                unit_slice = slice(local_units * rank, local_units * (rank + 1))\n",
    "                \n",
    "                mlp_layer[0].down_proj.weight.data[...] = ref_model.model.layers[i].mlp.down_proj.weight.data[:, unit_slice]\n",
    "                mlp_layer[0].up_proj.weight.data[...] = ref_model.model.layers[i].mlp.up_proj.weight.data[unit_slice]\n",
    "                mlp_layer[0].gate_proj.weight.data[...] = ref_model.model.layers[i].mlp.gate_proj.weight.data[unit_slice]\n",
    "                tp_model.model.layers[i].mlp = mlp_layer\n",
    "    \n",
    "    dist.barrier()\n",
    "    tp_input = input.detach().requires_grad_(True)\n",
    "    tp_output = tp_model(inputs_embeds=tp_input).logits\n",
    "    if rank == 0:\n",
    "        print(f\"\\nReference outputs ({rank=}):\", ref_output.data, flush=True)\n",
    "    for i in range(world_size):\n",
    "        dist.barrier()\n",
    "        if i != rank: continue\n",
    "        print(f\"TParallel outputs ({rank=}):\", tp_output.data, flush=True)\n",
    "        assert torch.allclose(tp_output, ref_output, atol=1e-4), f\"output mismatch on {rank=}\"\n",
    "    \n",
    "    dist.barrier()  # test 2: backward w.r.t. inputs\n",
    "    assert tp_input.grad is None\n",
    "    tp_output.mean().backward()\n",
    "    if rank == 0:\n",
    "        print(f\"\\nReference input grad ({rank=}):\", ref_input_grad, flush=True)\n",
    "    for i in range(world_size):\n",
    "        dist.barrier()\n",
    "        if i != rank: continue\n",
    "        print(f\"TParallel input grad ({rank=}):\", tp_input.grad.data, flush=True)\n",
    "        assert torch.allclose(tp_input.grad, ref_input_grad, atol=1e-4), f\"input_grad mismatch on {rank=}\"\n",
    "    \n",
    "    dist.barrier()\n",
    "    text = \"A quick brown\"\n",
    "    input_ids = tokenizer(text, return_tensors='pt')[\"input_ids\"]\n",
    "    for i in range(10):\n",
    "        with torch.no_grad():\n",
    "            new_token = tp_model(input_ids).logits[0, -1].argmax(-1)\n",
    "            input_ids = torch.cat([input_ids, new_token.view(1, 1)], dim=1)\n",
    "        # print(end=tokenizer.decode(new_token), flush=True)\n",
    "    final_text = tokenizer.decode(input_ids[0])\n",
    "    print(end=f\"\\n\\nText generation ({rank=}):\\n{final_text}\\n\", flush=True)\n",
    "    dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "plldOGaGo8A-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "\n",
      "Reference outputs (rank=0): tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
      "         [ 8.7207,  7.6674,  7.7454,  ..., -1.3887, -1.3885, -1.3885],\n",
      "         [ 9.7538,  7.3480,  5.6021,  ..., -0.3843, -0.3842, -0.3834],\n",
      "         [10.0018,  7.8891,  5.5960,  ..., -2.3212, -2.3206, -2.3199],\n",
      "         [12.8186, 10.7330,  8.5906,  ..., -1.0288, -1.0301, -1.0296]]])\n",
      "TParallel outputs (rank=0): tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
      "         [ 8.7207,  7.6674,  7.7454,  ..., -1.3887, -1.3885, -1.3885],\n",
      "         [ 9.7538,  7.3480,  5.6021,  ..., -0.3843, -0.3842, -0.3834],\n",
      "         [10.0018,  7.8891,  5.5960,  ..., -2.3212, -2.3206, -2.3199],\n",
      "         [12.8186, 10.7330,  8.5906,  ..., -1.0288, -1.0301, -1.0296]]])\n",
      "TParallel outputs (rank=1): tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
      "         [ 8.7207,  7.6674,  7.7454,  ..., -1.3887, -1.3885, -1.3885],\n",
      "         [ 9.7538,  7.3480,  5.6021,  ..., -0.3843, -0.3842, -0.3834],\n",
      "         [10.0018,  7.8891,  5.5960,  ..., -2.3212, -2.3206, -2.3199],\n",
      "         [12.8186, 10.7330,  8.5906,  ..., -1.0288, -1.0301, -1.0296]]])\n",
      "\n",
      "Reference input grad (rank=0): tensor([[[-0.2228,  0.3732, -0.4468,  ..., -0.0658,  0.6200, -0.4789],\n",
      "         [-0.1781, -0.0525, -0.1780,  ..., -0.0826,  0.0628, -0.0864],\n",
      "         [-0.2818,  0.1264, -0.1213,  ..., -0.0704, -0.1102,  0.0608],\n",
      "         [ 0.8313,  0.0922, -0.1806,  ...,  0.0285,  0.2226, -0.3346],\n",
      "         [-0.1841,  0.2162, -0.1329,  ..., -0.4778, -0.3304, -0.9067]]])\n",
      "TParallel input grad (rank=0): tensor([[[-0.2228,  0.3732, -0.4468,  ..., -0.0658,  0.6200, -0.4789],\n",
      "         [-0.1781, -0.0525, -0.1780,  ..., -0.0826,  0.0628, -0.0864],\n",
      "         [-0.2818,  0.1264, -0.1213,  ..., -0.0704, -0.1102,  0.0608],\n",
      "         [ 0.8313,  0.0922, -0.1806,  ...,  0.0285,  0.2226, -0.3346],\n",
      "         [-0.1841,  0.2162, -0.1329,  ..., -0.4778, -0.3304, -0.9067]]])\n",
      "TParallel input grad (rank=1): tensor([[[-0.2228,  0.3732, -0.4468,  ..., -0.0658,  0.6200, -0.4789],\n",
      "         [-0.1781, -0.0525, -0.1780,  ..., -0.0826,  0.0628, -0.0864],\n",
      "         [-0.2818,  0.1264, -0.1213,  ..., -0.0704, -0.1102,  0.0608],\n",
      "         [ 0.8313,  0.0922, -0.1806,  ...,  0.0285,  0.2226, -0.3346],\n",
      "         [-0.1841,  0.2162, -0.1329,  ..., -0.4778, -0.3304, -0.9067]]])\n",
      "\n",
      "\n",
      "Text generation (rank=0):\n",
      "<|begin_of_text|>A quick brownie recipe that is easy to make and tastes great\n",
      "\n",
      "\n",
      "Text generation (rank=1):\n",
      "<|begin_of_text|>A quick brownie recipe that is easy to make and tastes great\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=1 torchrun --nproc_per_node 2 tensor_parallel_llama.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxBHNBsDq2r2"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "### Using [`torch.distributed.tensor`](https://pytorch.org/docs/stable/distributed.tensor.html)\n",
    "\n",
    "PyTorch has an in-built functionality called [DTensor](https://pytorch.org/docs/stable/distributed.tensor.html), designed to help implementing tensor-level parallelism with various sharding strategies. This includes Tensor parallelism itself, as well as other techniques such as Sequence Parallelism, as they are both, essentially, parallelism across different tensor dimensions.\n",
    "\n",
    "__Task 3 (1 point):__ Your next task will be to replicate your previous code (llama inference) using DTensor instead of manual AllReduce. We recommend you start by skimming the [documentation for DTensor](https://pytorch.org/docs/stable/distributed.tensor.html) to learn the interface and [the minimal example](https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/tensor_parallel_example.py) to learn how to put the pieces together.\n",
    "\n",
    "\n",
    "We recommend that you dedicate some time to learn and play with it before you proceed to parallelize Llama.\n",
    "\n",
    "The main objective is the same as in the previous task - run .generate with DTensor - and then compare it against the manual implementation. **Please report at least some speed comparison for forward and backward passes between this and the previous task.** If absolutely impossible (e.g. you don't have multiple gpus), we can accept a fallback assignment of implementing basic training: overfit the model to a single batch (like task 5 below) and demonstrate that it works - if you choose this option, say so in bold, large-font letters somewhere where the grader can see.\n",
    "\n",
    "But first, here's a quick demo of using DTensor for simple matrix multiplication - meant as a testbed for your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed._tensor import DTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtvFF26mDWH2",
    "outputId": "9ddddac6-65aa-4341-f12f-e884a26f0bea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tensor_parallel_mlp_dtensor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tensor_parallel_mlp_dtensor.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed._tensor import DTensor, DeviceMesh, Replicate, Shard\n",
    "import torch.distributed.tensor.parallel as tp\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):  # same module, but with smaller dims for quick prototyping\n",
    "    def __init__(self, hidden_size: int = 1024, intermediate_size: int = 4096):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.down_proj(F.silu(self.gate_proj(input)) * self.up_proj(input))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dist.init_process_group(\"gloo\")  # use nccl for cuda devices\n",
    "    torch.manual_seed(1337)          # init weights equally on all ranks\n",
    "    rank, world_size = dist.get_rank(), dist.get_world_size()\n",
    "\n",
    "    # Initialize device mesh for tensor parallelism\n",
    "    device_mesh = init_device_mesh(device_type=\"cpu\", mesh_shape=(world_size,))  # use \"cuda\" for GPU\n",
    "\n",
    "    # Create reference module for comparison\n",
    "    ref_module = nn.Sequential(nn.RMSNorm(1024), LlamaMLP())\n",
    "\n",
    "    input = torch.randn(1, 1024, requires_grad=True)\n",
    "    ref_output = ref_module(input)\n",
    "    ref_output.sum().backward()\n",
    "    ref_input_grad = input.grad.clone()\n",
    "\n",
    "    # Create tensor parallel module (we wrap ref_module instead of copying)\n",
    "    tp_module = tp.parallelize_module(\n",
    "        ref_module,\n",
    "        device_mesh,\n",
    "        parallelize_plan={  # define parallelism type for each module\n",
    "            # up_proj and gate_proj are column-wise parallel (sharded across outputs);\n",
    "            \"1.up_proj\": tp.ColwiseParallel(),\n",
    "            \"1.gate_proj\": tp.ColwiseParallel(),\n",
    "            # down_proj is row-wise parallel (sharded across input dim)\n",
    "            \"1.down_proj\": tp.RowwiseParallel(),\n",
    "          },  # note: RMSNorm is simply replicated across all devices - hence, we skip it\n",
    "    )\n",
    "    if rank == 0:  # Note: no need to copy weight chunks manually: DTensor handles parameter sharding for us\n",
    "      for name, param in tp_module.named_parameters():\n",
    "        print(f\"{name=},\\ttype={type(param.data)}\\tglobal shape={param.shape},\\tlocal shape={param._local_tensor.shape if hasattr(param, '_local_tensor') else param.shape}\")\n",
    "\n",
    "    dist.barrier()  # Test forward and backward pass with Tensor Parallelism\n",
    "    tp_input = input.detach().requires_grad_(True)\n",
    "    tp_output = tp_module(tp_input)\n",
    "    tp_output.sum().backward()\n",
    "    tp_output = tp_output.trigger_wait()  # convert from AsyncCollectiveTensor to regular torch tensor\n",
    "    if rank == 0:\n",
    "        print(f\"\\nReference outputs ({rank=}):\", ref_output.data, flush=True)\n",
    "        print(f\"TParallel outputs ({rank=}):\", tp_output.data, flush=True)\n",
    "        print(f\"\\nReference input grad ({rank=}):\", ref_input_grad, flush=True)\n",
    "        print(f\"TParallel input grad ({rank=}):\", tp_input.grad, flush=True)\n",
    "    dist.barrier()\n",
    "    assert torch.allclose(tp_output, ref_output, atol=1e-6), f\"output mismatch on {rank=}\"\n",
    "    assert torch.allclose(tp_input.grad, ref_input_grad, atol=1e-6), f\"input_grad mismatch on {rank=}\"\n",
    "    print(end=f\"Tests passed ({rank=})\\n\", flush=True); dist.barrier()\n",
    "\n",
    "# fun fact: 90% of the code above was generated by grok-3 for prompt \"Please rewrite the following code using torch.distributed.tensor ```python <paste MLP code here>```\"\n",
    "# the remaining 10% are nasty bugfixes that took 99% of assignment preparation time. Do not trust the shogoths yet :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rbGsyCMHDtXY",
    "outputId": "337e86a1-d9fa-48e2-aff0-2f0089b56ff2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank [Gloo] Rank 10 is connected to  is connected to 11 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 11\n",
      "\n",
      "name='0.weight',\ttype=<class 'torch.Tensor'>\tglobal shape=torch.Size([1024]),\tlocal shape=torch.Size([1024])\n",
      "name='1.gate_proj.weight',\ttype=<class 'torch.distributed.tensor.DTensor'>\tglobal shape=torch.Size([4096, 1024]),\tlocal shape=torch.Size([2048, 1024])\n",
      "name='1.up_proj.weight',\ttype=<class 'torch.distributed.tensor.DTensor'>\tglobal shape=torch.Size([4096, 1024]),\tlocal shape=torch.Size([2048, 1024])\n",
      "name='1.down_proj.weight',\ttype=<class 'torch.distributed.tensor.DTensor'>\tglobal shape=torch.Size([1024, 4096]),\tlocal shape=torch.Size([1024, 2048])\n",
      "\n",
      "Reference outputs (rank=0): tensor([[ 0.0102,  0.0432, -0.0467,  ...,  0.0798, -0.0179,  0.0527]])\n",
      "TParallel outputs (rank=0): tensor([[ 0.0102,  0.0432, -0.0467,  ...,  0.0798, -0.0179,  0.0527]])\n",
      "\n",
      "Reference input grad (rank=0): tensor([[ 0.1543, -0.0858, -0.0882,  ..., -0.2082,  0.0298,  0.2388]])\n",
      "TParallel input grad (rank=0): tensor([[ 0.1543, -0.0858, -0.0882,  ..., -0.2082,  0.0298,  0.2388]])\n",
      "Tests passed (rank=0)\n",
      "Tests passed (rank=1)\n",
      "[rank0]:[W223 08:06:16.947047995 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=1 torchrun --nproc_per_node 2 tensor_parallel_mlp_dtensor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qoA-ZAW4doMy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tensor_parallel_attn_dtensor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tensor_parallel_attn_dtensor.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LlamaConfig, LlamaAttention, LlamaRotaryEmbedding\n",
    "from torch import nn\n",
    "import torch\n",
    "from typing import Dict, Callable, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed._tensor import DTensor, DeviceMesh, Replicate, Shard\n",
    "import torch.distributed.tensor.parallel as tp\n",
    "\n",
    "\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B\"  # for testing (but not grading!), you may want to use Maykeye/TinyLLama-v0\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def sdpa_attention_forward(\n",
    "    module: torch.nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    dropout: float = 0.0,\n",
    "    scaling: Optional[float] = None,\n",
    "    is_causal: Optional[bool] = None,\n",
    "    **kwargs,\n",
    ") -> Tuple[torch.Tensor, None]:\n",
    "    if hasattr(module, \"num_key_value_groups\"):\n",
    "        key = repeat_kv(key, module.num_key_value_groups)\n",
    "        value = repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    causal_mask = attention_mask\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n",
    "\n",
    "    query = query.contiguous()\n",
    "    key = key.contiguous()\n",
    "    value = value.contiguous()\n",
    "\n",
    "    if is_causal is None:\n",
    "        is_causal = causal_mask is None and query.shape[2] > 1\n",
    "\n",
    "    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):\n",
    "        is_causal = is_causal.item()\n",
    "\n",
    "    # print('query', query.shape)\n",
    "    # print('key', key.shape)\n",
    "    # print('value', value.shape)\n",
    "    \n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=dropout,\n",
    "        scale=scaling,\n",
    "        is_causal=is_causal,\n",
    "    )\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, None\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class MyLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # print('query_states', query_states.shape)\n",
    "        # print('key_states', key_states.shape)\n",
    "        # print('value_states', value_states.shape)\n",
    "        \n",
    "        attn_output, attn_weights = sdpa_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "        )\n",
    "\n",
    "        # print('attn_output', attn_output.shape)\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class ComputeWithAllReduce(torch.autograd.Function):\n",
    "    @staticmethod  # fun fact: torch.distributed.nn has differentiable all_reduce!\n",
    "    def forward(ctx, tp_shard: nn.Module, input: torch.Tensor, **kwargs):\n",
    "        input = input.detach().requires_grad_(input.requires_grad)\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx._tp_shard = tp_shard\n",
    "        output = tp_shard(input, **kwargs)\n",
    "        dist.all_reduce(output)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        with torch.enable_grad():\n",
    "          output = ctx._tp_shard(ctx.saved_tensors[0])\n",
    "          output.backward(grad_output)\n",
    "        dist.all_reduce(ctx.saved_tensors[0].grad)\n",
    "        return None, ctx.saved_tensors[0].grad\n",
    "\n",
    "\n",
    "class ReduceWrapper(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        fn = lambda x: self.module(x, **kwargs)\n",
    "        return ComputeWithAllReduce.apply(fn, input)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dist.init_process_group(\"gloo\")   # use nccl for cuda devices\n",
    "    torch.manual_seed(1337)           # init weights equally on all ranks\n",
    "    rank, world_size = dist.get_rank(), dist.get_world_size()\n",
    "\n",
    "    device_mesh = init_device_mesh(device_type=\"cpu\", mesh_shape=(world_size,))\n",
    "\n",
    "    config = LlamaConfig.from_pretrained(MODEL_NAME)\n",
    "    ref_module = MyLlamaAttention(config, layer_idx=5).cpu()\n",
    "    rotary_emb = LlamaRotaryEmbedding(config).cpu()\n",
    "\n",
    "    input = torch.randn(1, 128, 2048, requires_grad=True, device='cpu')\n",
    "    position_embeddings = rotary_emb(input, position_ids=torch.arange(128)[None].cpu())\n",
    "    extra_inputs = dict(attention_mask=None, position_embeddings=position_embeddings)  # <-- OPTIONAL: either design additional inputs here, as in the reference above\n",
    "\n",
    "    ref_output = ref_module(input, **extra_inputs).cpu()\n",
    "    ref_output.sum().backward()\n",
    "    ref_input_grad = input.grad.clone()\n",
    "\n",
    "    tp_module = tp.parallelize_module(\n",
    "        ref_module,\n",
    "        device_mesh,\n",
    "        parallelize_plan={\n",
    "            \"q_proj\": tp.ColwiseParallel(),\n",
    "            \"k_proj\": tp.ColwiseParallel(),\n",
    "            \"v_proj\": tp.ColwiseParallel(),\n",
    "            \"o_proj\": tp.RowwiseParallel(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if rank == 0:\n",
    "        for name, param in tp_module.named_parameters():\n",
    "            print(f\"{name=},\\ttype={type(param.data)}\\tglobal shape={param.shape},\\tlocal shape={param._local_tensor.shape if hasattr(param, '_local_tensor') else param.shape}\")\n",
    "    \n",
    "    dist.barrier()\n",
    "    tp_input = input.detach().requires_grad_(True)\n",
    "    tp_output = tp_module(tp_input, **extra_inputs)\n",
    "    tp_output.sum().backward()\n",
    "    tp_output = tp_output.trigger_wait()\n",
    "    if rank == 0:\n",
    "        print(f\"\\nReference outputs ({rank=}):\", ref_output.data, flush=True)\n",
    "        print(f\"TParallel outputs ({rank=}):\", tp_output.data, flush=True)\n",
    "        print(f\"\\nReference input grad ({rank=}):\", ref_input_grad, flush=True)\n",
    "        print(f\"TParallel input grad ({rank=}):\", tp_input.grad, flush=True)\n",
    "    dist.barrier()\n",
    "    assert torch.allclose(tp_output, ref_output, atol=1e-6), f\"output mismatch on {rank=}\"\n",
    "    assert torch.allclose(tp_input.grad, ref_input_grad, atol=1e-6), f\"input_grad mismatch on {rank=}\"\n",
    "    print(end=f\"Tests passed ({rank=})\\n\", flush=True); dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DYPmr0xCfgEK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1[Gloo] Rank \n",
      "1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0[Gloo] Rank  is connected to 11 is connected to  peer ranks. 1Expected number of connected peer ranks is :  peer ranks. 1Expected number of connected peer ranks is : \n",
      "1\n",
      "name='q_proj.weight',\ttype=<class 'torch.distributed.tensor.DTensor'>\tglobal shape=torch.Size([2048, 2048]),\tlocal shape=torch.Size([1024, 2048])\n",
      "name='k_proj.weight',\ttype=<class 'torch.distributed.tensor.DTensor'>\tglobal shape=torch.Size([512, 2048]),\tlocal shape=torch.Size([256, 2048])\n",
      "name='v_proj.weight',\ttype=<class 'torch.distributed.tensor.DTensor'>\tglobal shape=torch.Size([512, 2048]),\tlocal shape=torch.Size([256, 2048])\n",
      "name='o_proj.weight',\ttype=<class 'torch.distributed.tensor.DTensor'>\tglobal shape=torch.Size([2048, 2048]),\tlocal shape=torch.Size([2048, 1024])\n",
      "\n",
      "Reference outputs (rank=0): tensor([[[-0.1457, -0.3905, -0.6217,  ..., -0.4383, -0.0785, -0.1008],\n",
      "         [-0.2244,  0.0917, -0.3745,  ..., -0.4855, -0.0364,  0.0169],\n",
      "         [-0.0185,  0.0349, -0.2067,  ..., -0.3230,  0.1927,  0.0192],\n",
      "         ...,\n",
      "         [ 0.0261, -0.0290,  0.0117,  ...,  0.0818, -0.0607, -0.0267],\n",
      "         [ 0.0211, -0.0107, -0.0039,  ...,  0.1037, -0.0651, -0.0218],\n",
      "         [ 0.0070, -0.0101,  0.0133,  ...,  0.0769, -0.0663, -0.0287]]])\n",
      "TParallel outputs (rank=0): tensor([[[-0.1457, -0.3905, -0.6217,  ..., -0.4383, -0.0785, -0.1008],\n",
      "         [-0.2244,  0.0917, -0.3745,  ..., -0.4855, -0.0364,  0.0169],\n",
      "         [-0.0185,  0.0349, -0.2067,  ..., -0.3230,  0.1927,  0.0192],\n",
      "         ...,\n",
      "         [ 0.0261, -0.0290,  0.0117,  ...,  0.0818, -0.0607, -0.0267],\n",
      "         [ 0.0211, -0.0107, -0.0039,  ...,  0.1037, -0.0651, -0.0218],\n",
      "         [ 0.0070, -0.0101,  0.0133,  ...,  0.0769, -0.0663, -0.0287]]])\n",
      "\n",
      "Reference input grad (rank=0): tensor([[[-1.7667,  1.2170,  0.9256,  ..., -0.9333, -0.1490, -0.7688],\n",
      "         [-1.2510,  1.0999,  0.6966,  ..., -0.9081, -0.1355, -0.9720],\n",
      "         [-1.1445,  0.8347,  0.6998,  ..., -0.4806, -0.3040, -0.7971],\n",
      "         ...,\n",
      "         [-0.0112, -0.0040,  0.0066,  ..., -0.0104, -0.0092, -0.0105],\n",
      "         [-0.0139, -0.0105,  0.0081,  ..., -0.0134, -0.0076, -0.0106],\n",
      "         [-0.0174, -0.0066,  0.0035,  ..., -0.0092, -0.0088, -0.0097]]])\n",
      "TParallel input grad (rank=0): tensor([[[-1.7667,  1.2170,  0.9256,  ..., -0.9333, -0.1490, -0.7688],\n",
      "         [-1.2510,  1.0999,  0.6966,  ..., -0.9081, -0.1355, -0.9720],\n",
      "         [-1.1445,  0.8347,  0.6998,  ..., -0.4806, -0.3040, -0.7971],\n",
      "         ...,\n",
      "         [-0.0112, -0.0040,  0.0066,  ..., -0.0104, -0.0092, -0.0105],\n",
      "         [-0.0139, -0.0105,  0.0081,  ..., -0.0134, -0.0076, -0.0106],\n",
      "         [-0.0174, -0.0066,  0.0035,  ..., -0.0092, -0.0088, -0.0097]]])\n",
      "Tests passed (rank=0)\n",
      "Tests passed (rank=1)\n",
      "[rank0]:[W223 08:06:45.374298792 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=1 torchrun --nproc_per_node 2 tensor_parallel_attn_dtensor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tensor_parallel_llama_dtensor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tensor_parallel_llama_dtensor.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LlamaConfig, LlamaAttention, LlamaRotaryEmbedding\n",
    "from torch import nn\n",
    "import torch\n",
    "from typing import Dict, Callable, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed._tensor import DTensor, DeviceMesh, Replicate, Shard\n",
    "import torch.distributed.tensor.parallel as tp\n",
    "import transformers\n",
    "\n",
    "\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B\"  # for testing (but not grading!), you may want to use Maykeye/TinyLLama-v0\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def sdpa_attention_forward(\n",
    "    module: torch.nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    dropout: float = 0.0,\n",
    "    scaling: Optional[float] = None,\n",
    "    is_causal: Optional[bool] = None,\n",
    "    **kwargs,\n",
    ") -> Tuple[torch.Tensor, None]:\n",
    "    if hasattr(module, \"num_key_value_groups\"):\n",
    "        key = repeat_kv(key, module.num_key_value_groups)\n",
    "        value = repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    causal_mask = attention_mask\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n",
    "\n",
    "    query = query.contiguous()\n",
    "    key = key.contiguous()\n",
    "    value = value.contiguous()\n",
    "\n",
    "    if is_causal is None:\n",
    "        is_causal = causal_mask is None and query.shape[2] > 1\n",
    "\n",
    "    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):\n",
    "        is_causal = is_causal.item()\n",
    "\n",
    "    # print('query', query.shape)\n",
    "    # print('key', key.shape)\n",
    "    # print('value', value.shape)\n",
    "    \n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=dropout,\n",
    "        scale=scaling,\n",
    "        is_causal=is_causal,\n",
    "    )\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, None\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class MyLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # print('query_states', query_states.shape)\n",
    "        # print('key_states', key_states.shape)\n",
    "        # print('value_states', value_states.shape)\n",
    "        \n",
    "        attn_output, attn_weights = sdpa_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "        )\n",
    "\n",
    "        # print('attn_output', attn_output.shape)\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class ComputeWithAllReduce(torch.autograd.Function):\n",
    "    @staticmethod  # fun fact: torch.distributed.nn has differentiable all_reduce!\n",
    "    def forward(ctx, tp_shard: nn.Module, input: torch.Tensor, **kwargs):\n",
    "        input = input.detach().requires_grad_(input.requires_grad)\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx._tp_shard = tp_shard\n",
    "        output = tp_shard(input, **kwargs)\n",
    "        dist.all_reduce(output)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        with torch.enable_grad():\n",
    "          output = ctx._tp_shard(ctx.saved_tensors[0])\n",
    "          output.backward(grad_output)\n",
    "        dist.all_reduce(ctx.saved_tensors[0].grad)\n",
    "        return None, ctx.saved_tensors[0].grad\n",
    "\n",
    "\n",
    "class ReduceWrapper(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        fn = lambda x: self.module(x, **kwargs)\n",
    "        return ComputeWithAllReduce.apply(fn, input)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dist.init_process_group(\"nccl\")   # use nccl for cuda devices\n",
    "    torch.manual_seed(1337)           # init weights equally on all ranks\n",
    "    rank, world_size = dist.get_rank(), dist.get_world_size()\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    device_mesh = init_device_mesh(device_type=\"cuda\", mesh_shape=(world_size,))\n",
    "\n",
    "    MODEL_NAME = \"unsloth/Llama-3.2-1B\"  # for testing (but not grading!), you may want to use Maykeye/TinyLLama-v0\n",
    "    config = LlamaConfig.from_pretrained(MODEL_NAME)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    ref_model = transformers.LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32).cuda()  # <-- you are allowed to switch to bf16\n",
    "    prompt = \"A quick brown fox\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt')[\"input_ids\"].cuda()\n",
    "    input = ref_model.model.embed_tokens(input_ids).detach().requires_grad_(True)\n",
    "    ref_output = ref_model(inputs_embeds=input).logits\n",
    "    ref_output.mean().backward()\n",
    "    ref_input_grad = input.grad.clone()\n",
    "\n",
    "    parallelize_plan = {}\n",
    "\n",
    "    for i in range(len(ref_model.model.layers)):\n",
    "        parallelize_plan[f\"model.layers.{i}.self_attn.q_proj\"] = tp.ColwiseParallel()\n",
    "        parallelize_plan[f\"model.layers.{i}.self_attn.k_proj\"] = tp.ColwiseParallel()\n",
    "        parallelize_plan[f\"model.layers.{i}.self_attn.v_proj\"] = tp.ColwiseParallel()\n",
    "        parallelize_plan[f\"model.layers.{i}.self_attn.o_proj\"] = tp.RowwiseParallel()\n",
    "        parallelize_plan[f\"model.layers.{i}.mlp.gate_proj\"] = tp.ColwiseParallel()\n",
    "        parallelize_plan[f\"model.layers.{i}.mlp.down_proj\"] = tp.RowwiseParallel()\n",
    "        parallelize_plan[f\"model.layers.{i}.mlp.up_proj\"] = tp.ColwiseParallel()\n",
    "        \n",
    "    \n",
    "    tp_module = tp.parallelize_module(\n",
    "        ref_model,\n",
    "        device_mesh,\n",
    "        parallelize_plan=parallelize_plan\n",
    "    )\n",
    "\n",
    "    # if rank == 0:\n",
    "    #     for name, param in tp_module.named_parameters():\n",
    "    #         print(f\"{name=},\\ttype={type(param.data)}\\tglobal shape={param.shape},\\tlocal shape={param._local_tensor.shape if hasattr(param, '_local_tensor') else param.shape}\")\n",
    "    \n",
    "    dist.barrier()\n",
    "    tp_input = input.detach().requires_grad_(True).cuda()\n",
    "    tp_output = tp_module(inputs_embeds=tp_input).logits\n",
    "    tp_output.mean().backward()\n",
    "    if rank == 0:\n",
    "        print(f\"\\nReference outputs ({rank=}):\", ref_output.data, flush=True)\n",
    "        print(f\"TParallel outputs ({rank=}):\", tp_output.data, flush=True)\n",
    "        print(f\"\\nReference input grad ({rank=}):\", ref_input_grad, flush=True)\n",
    "        print(f\"TParallel input grad ({rank=}):\", tp_input.grad, flush=True)\n",
    "    dist.barrier()\n",
    "    assert torch.allclose(tp_output, ref_output, atol=1e-3), f\"output mismatch on {rank=}\"\n",
    "    assert torch.allclose(tp_input.grad, ref_input_grad, atol=1e-3), f\"input_grad mismatch on {rank=}\"\n",
    "    print(end=f\"Tests passed ({rank=})\\n\", flush=True); dist.barrier()\n",
    "\n",
    "    dist.barrier()\n",
    "    text = \"A quick brown\"\n",
    "    input_ids = tokenizer(text, return_tensors='pt')[\"input_ids\"].cuda()\n",
    "    for i in range(10):\n",
    "        with torch.no_grad():\n",
    "            new_token = tp_module(input_ids).logits[0, -1].argmax(-1)\n",
    "            input_ids = torch.cat([input_ids, new_token.view(1, 1)], dim=1)\n",
    "    final_text = tokenizer.decode(input_ids[0])\n",
    "    print(end=f\"\\n\\nText generation ({rank=}):\\n{final_text}\\n\", flush=True)\n",
    "    dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  return func(*args, **kwargs)\n",
      "\n",
      "Reference outputs (rank=0): tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
      "         [ 8.7207,  7.6674,  7.7454,  ..., -1.3887, -1.3885, -1.3885],\n",
      "         [ 9.7538,  7.3480,  5.6021,  ..., -0.3843, -0.3842, -0.3834],\n",
      "         [10.0018,  7.8891,  5.5960,  ..., -2.3212, -2.3206, -2.3199],\n",
      "         [12.8186, 10.7330,  8.5906,  ..., -1.0288, -1.0301, -1.0296]]],\n",
      "       device='cuda:0')\n",
      "TParallel outputs (rank=0): tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
      "         [ 8.7207,  7.6674,  7.7454,  ..., -1.3887, -1.3885, -1.3885],\n",
      "         [ 9.7538,  7.3480,  5.6021,  ..., -0.3843, -0.3842, -0.3834],\n",
      "         [10.0018,  7.8891,  5.5960,  ..., -2.3212, -2.3206, -2.3199],\n",
      "         [12.8186, 10.7330,  8.5906,  ..., -1.0288, -1.0301, -1.0296]]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Reference input grad (rank=0): tensor([[[-0.2228,  0.3732, -0.4468,  ..., -0.0658,  0.6200, -0.4789],\n",
      "         [-0.1781, -0.0525, -0.1780,  ..., -0.0826,  0.0628, -0.0864],\n",
      "         [-0.2818,  0.1264, -0.1213,  ..., -0.0704, -0.1102,  0.0608],\n",
      "         [ 0.8313,  0.0922, -0.1806,  ...,  0.0285,  0.2226, -0.3346],\n",
      "         [-0.1841,  0.2162, -0.1329,  ..., -0.4778, -0.3304, -0.9067]]],\n",
      "       device='cuda:0')\n",
      "TParallel input grad (rank=0): tensor([[[-0.2227,  0.3732, -0.4468,  ..., -0.0658,  0.6200, -0.4789],\n",
      "         [-0.1781, -0.0525, -0.1780,  ..., -0.0827,  0.0628, -0.0864],\n",
      "         [-0.2818,  0.1264, -0.1213,  ..., -0.0705, -0.1102,  0.0608],\n",
      "         [ 0.8312,  0.0922, -0.1806,  ...,  0.0285,  0.2226, -0.3346],\n",
      "         [-0.1841,  0.2162, -0.1329,  ..., -0.4778, -0.3304, -0.9067]]],\n",
      "       device='cuda:0')\n",
      "Tests passed (rank=0)\n",
      "Tests passed (rank=1)\n",
      "\n",
      "\n",
      "Text generation (rank=0):\n",
      "<|begin_of_text|>A quick brownie recipe that is easy to make and tastes great\n",
      "\n",
      "\n",
      "Text generation (rank=1):\n",
      "<|begin_of_text|>A quick brownie recipe that is easy to make and tastes great\n",
      "[rank0]:[W223 08:32:24.939919021 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "CPU times: user 141 ms, sys: 215 ms, total: 356 ms\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!OMP_NUM_THREADS=1 torchrun --nproc_per_node 2 tensor_parallel_llama_dtensor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tensor_parallel_llama_cuda.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tensor_parallel_llama_cuda.py\n",
    "import torch\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LlamaConfig, LlamaAttention, LlamaRotaryEmbedding\n",
    "from torch import nn\n",
    "import torch\n",
    "from typing import Dict, Callable, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B\"  # for testing (but not grading!), you may want to use Maykeye/TinyLLama-v0\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def sdpa_attention_forward(\n",
    "    module: torch.nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    dropout: float = 0.0,\n",
    "    scaling: Optional[float] = None,\n",
    "    is_causal: Optional[bool] = None,\n",
    "    **kwargs,\n",
    ") -> Tuple[torch.Tensor, None]:\n",
    "    if hasattr(module, \"num_key_value_groups\"):\n",
    "        key = repeat_kv(key, module.num_key_value_groups)\n",
    "        value = repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    causal_mask = attention_mask\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n",
    "\n",
    "    query = query.contiguous()\n",
    "    key = key.contiguous()\n",
    "    value = value.contiguous()\n",
    "\n",
    "    if is_causal is None:\n",
    "        is_causal = causal_mask is None and query.shape[2] > 1\n",
    "\n",
    "    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):\n",
    "        is_causal = is_causal.item()\n",
    "\n",
    "    # print('query', query.shape)\n",
    "    # print('key', key.shape)\n",
    "    # print('value', value.shape)\n",
    "    \n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=dropout,\n",
    "        scale=scaling,\n",
    "        is_causal=is_causal,\n",
    "    )\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, None\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class MyLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor], **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # print('query_states', query_states.shape)\n",
    "        # print('key_states', key_states.shape)\n",
    "        # print('value_states', value_states.shape)\n",
    "        \n",
    "        attn_output, attn_weights = sdpa_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "        )\n",
    "\n",
    "        # print('attn_output', attn_output.shape)\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class ComputeWithAllReduce(torch.autograd.Function):\n",
    "    @staticmethod  # fun fact: torch.distributed.nn has differentiable all_reduce!\n",
    "    def forward(ctx, tp_shard: nn.Module, input: torch.Tensor, **kwargs):\n",
    "        input = input.detach().requires_grad_(input.requires_grad)\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx._tp_shard = tp_shard\n",
    "        output = tp_shard(input, **kwargs)\n",
    "        dist.all_reduce(output)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        with torch.enable_grad():\n",
    "          output = ctx._tp_shard(ctx.saved_tensors[0])\n",
    "          output.backward(grad_output)\n",
    "        dist.all_reduce(ctx.saved_tensors[0].grad)\n",
    "        return None, ctx.saved_tensors[0].grad\n",
    "\n",
    "\n",
    "class ReduceWrapper(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, hidden_states=None, **kwargs):\n",
    "        fn = lambda x: self.module(x, **kwargs)\n",
    "        output = ComputeWithAllReduce.apply(fn, hidden_states)\n",
    "        return output, None\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):  #  based on llama 3.1 8B configuration\n",
    "    def __init__(self, hidden_size: int = 4096, intermediate_size: int = 14336):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.down_proj(F.silu(self.gate_proj(input)) * self.up_proj(input))\n",
    "\n",
    "\n",
    "class ComputeWithAllReduce(torch.autograd.Function):\n",
    "    @staticmethod  # fun fact: torch.distributed.nn has differentiable all_reduce!\n",
    "    def forward(ctx, tp_shard: nn.Module, input: torch.Tensor):\n",
    "        input = input.detach().requires_grad_(input.requires_grad)\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx._tp_shard = tp_shard\n",
    "        output = tp_shard(input)\n",
    "        dist.all_reduce(output)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        with torch.enable_grad():\n",
    "          output = ctx._tp_shard(ctx.saved_tensors[0])\n",
    "          output.backward(grad_output)\n",
    "        dist.all_reduce(ctx.saved_tensors[0].grad)\n",
    "        return None, ctx.saved_tensors[0].grad\n",
    "\n",
    "\n",
    "class AllReduceModule(nn.Sequential):\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        return ComputeWithAllReduce.apply(super().forward, input)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dist.init_process_group(\"nccl\")   # use nccl for cuda devices\n",
    "    torch.manual_seed(1337)           # init weights equally on all ranks\n",
    "    rank, world_size = dist.get_rank(), dist.get_world_size()\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    for active_rank in range(world_size):\n",
    "        dist.barrier()\n",
    "        if rank != active_rank: continue\n",
    "        \n",
    "        MODEL_NAME = \"unsloth/Llama-3.2-1B\"  # for testing (but not grading!), you may want to use Maykeye/TinyLLama-v0\n",
    "        config = LlamaConfig.from_pretrained(MODEL_NAME)\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        ref_model = transformers.LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32).cuda()  # <-- you are allowed to switch to bf16\n",
    "        prompt = \"A quick brown fox\"\n",
    "        input_ids = tokenizer(prompt, return_tensors='pt')[\"input_ids\"].cuda()\n",
    "        input = ref_model.model.embed_tokens(input_ids).detach().requires_grad_(True)\n",
    "        ref_output = ref_model(inputs_embeds=input).logits\n",
    "        ref_output.mean().backward()\n",
    "        ref_input_grad = input.grad.clone()\n",
    "        \n",
    "        tp_model = deepcopy(ref_model)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(len(tp_model.model.layers)):\n",
    "                smaller_config = deepcopy(config)\n",
    "                smaller_config.num_attention_heads = config.num_attention_heads // world_size\n",
    "                smaller_config.num_key_value_heads = config.num_key_value_heads // world_size\n",
    "                layer = ReduceWrapper(MyLlamaAttention(smaller_config, layer_idx=i)).cuda()\n",
    "                local_units = config.head_dim * smaller_config.num_key_value_heads\n",
    "                unit_slice = slice(local_units * rank, local_units * (rank + 1))\n",
    "                layer.module.k_proj.weight.data = ref_model.model.layers[i].self_attn.k_proj.weight.data[unit_slice]\n",
    "                layer.module.v_proj.weight.data = ref_model.model.layers[i].self_attn.v_proj.weight.data[unit_slice]\n",
    "                local_units = config.head_dim * smaller_config.num_attention_heads\n",
    "                unit_slice = slice(local_units * rank, local_units * (rank + 1))\n",
    "                layer.module.o_proj.weight.data[...] = ref_model.model.layers[i].self_attn.o_proj.weight.data[:, unit_slice]\n",
    "                layer.module.q_proj.weight.data[...] = ref_model.model.layers[i].self_attn.q_proj.weight.data[unit_slice]\n",
    "                tp_model.model.layers[i].self_attn = layer\n",
    "\n",
    "                intermediate_size = ref_model.model.layers[i].mlp.down_proj.in_features\n",
    "                local_units = intermediate_size // world_size\n",
    "                \n",
    "                mlp_layer = AllReduceModule(LlamaMLP(hidden_size=config.hidden_size, intermediate_size=local_units)).cuda()\n",
    "                unit_slice = slice(local_units * rank, local_units * (rank + 1))\n",
    "                \n",
    "                mlp_layer[0].down_proj.weight.data[...] = ref_model.model.layers[i].mlp.down_proj.weight.data[:, unit_slice]\n",
    "                mlp_layer[0].up_proj.weight.data[...] = ref_model.model.layers[i].mlp.up_proj.weight.data[unit_slice]\n",
    "                mlp_layer[0].gate_proj.weight.data[...] = ref_model.model.layers[i].mlp.gate_proj.weight.data[unit_slice]\n",
    "                tp_model.model.layers[i].mlp = mlp_layer\n",
    "    \n",
    "    dist.barrier()\n",
    "    tp_input = input.detach().requires_grad_(True).cuda()\n",
    "    tp_output = tp_model(inputs_embeds=tp_input).logits\n",
    "    if rank == 0:\n",
    "        print(f\"\\nReference outputs ({rank=}):\", ref_output.data, flush=True)\n",
    "    for i in range(world_size):\n",
    "        dist.barrier()\n",
    "        if i != rank: continue\n",
    "        print(f\"TParallel outputs ({rank=}):\", tp_output.data, flush=True)\n",
    "        assert torch.allclose(tp_output, ref_output, atol=1e-3), f\"output mismatch on {rank=}\"\n",
    "    \n",
    "    dist.barrier()  # test 2: backward w.r.t. inputs\n",
    "    assert tp_input.grad is None\n",
    "    tp_output.mean().backward()\n",
    "    if rank == 0:\n",
    "        print(f\"\\nReference input grad ({rank=}):\", ref_input_grad, flush=True)\n",
    "    for i in range(world_size):\n",
    "        dist.barrier()\n",
    "        if i != rank: continue\n",
    "        print(f\"TParallel input grad ({rank=}):\", tp_input.grad.data, flush=True)\n",
    "        assert torch.allclose(tp_input.grad, ref_input_grad, atol=1e-3), f\"input_grad mismatch on {rank=}\"\n",
    "    \n",
    "    dist.barrier()\n",
    "    text = \"A quick brown\"\n",
    "    input_ids = tokenizer(text, return_tensors='pt')[\"input_ids\"].cuda()\n",
    "    for i in range(10):\n",
    "        with torch.no_grad():\n",
    "            new_token = tp_model(input_ids).logits[0, -1].argmax(-1)\n",
    "            input_ids = torch.cat([input_ids, new_token.view(1, 1)], dim=1)\n",
    "        # print(end=tokenizer.decode(new_token), flush=True)\n",
    "    final_text = tokenizer.decode(input_ids[0])\n",
    "    print(end=f\"\\n\\nText generation ({rank=}):\\n{final_text}\\n\", flush=True)\n",
    "    dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  return func(*args, **kwargs)\n",
      "[rank0]:[W223 08:34:04.227183287 ProcessGroupNCCL.cpp:5138] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()\n",
      "\n",
      "Reference outputs (rank=0): tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
      "         [ 8.7207,  7.6674,  7.7454,  ..., -1.3887, -1.3885, -1.3885],\n",
      "         [ 9.7538,  7.3480,  5.6021,  ..., -0.3843, -0.3842, -0.3834],\n",
      "         [10.0018,  7.8891,  5.5960,  ..., -2.3212, -2.3206, -2.3199],\n",
      "         [12.8186, 10.7330,  8.5906,  ..., -1.0288, -1.0301, -1.0296]]],\n",
      "       device='cuda:0')\n",
      "TParallel outputs (rank=0): tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
      "         [ 8.7207,  7.6674,  7.7454,  ..., -1.3887, -1.3885, -1.3885],\n",
      "         [ 9.7538,  7.3480,  5.6021,  ..., -0.3843, -0.3842, -0.3834],\n",
      "         [10.0018,  7.8891,  5.5960,  ..., -2.3212, -2.3206, -2.3199],\n",
      "         [12.8186, 10.7330,  8.5906,  ..., -1.0288, -1.0301, -1.0296]]],\n",
      "       device='cuda:0')\n",
      "TParallel outputs (rank=1): tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
      "         [ 8.7207,  7.6674,  7.7454,  ..., -1.3887, -1.3885, -1.3885],\n",
      "         [ 9.7538,  7.3480,  5.6021,  ..., -0.3843, -0.3842, -0.3834],\n",
      "         [10.0018,  7.8891,  5.5960,  ..., -2.3212, -2.3206, -2.3199],\n",
      "         [12.8186, 10.7330,  8.5906,  ..., -1.0288, -1.0301, -1.0296]]],\n",
      "       device='cuda:1')\n",
      "\n",
      "Reference input grad (rank=0): tensor([[[-0.2228,  0.3732, -0.4468,  ..., -0.0658,  0.6200, -0.4789],\n",
      "         [-0.1781, -0.0525, -0.1780,  ..., -0.0826,  0.0628, -0.0864],\n",
      "         [-0.2818,  0.1264, -0.1213,  ..., -0.0704, -0.1102,  0.0608],\n",
      "         [ 0.8313,  0.0922, -0.1806,  ...,  0.0285,  0.2226, -0.3346],\n",
      "         [-0.1841,  0.2162, -0.1329,  ..., -0.4778, -0.3304, -0.9067]]],\n",
      "       device='cuda:0')\n",
      "TParallel input grad (rank=0): tensor([[[-0.2227,  0.3731, -0.4468,  ..., -0.0658,  0.6200, -0.4789],\n",
      "         [-0.1781, -0.0525, -0.1780,  ..., -0.0827,  0.0628, -0.0864],\n",
      "         [-0.2818,  0.1264, -0.1213,  ..., -0.0705, -0.1102,  0.0608],\n",
      "         [ 0.8312,  0.0922, -0.1806,  ...,  0.0285,  0.2226, -0.3346],\n",
      "         [-0.1841,  0.2162, -0.1329,  ..., -0.4778, -0.3304, -0.9067]]],\n",
      "       device='cuda:0')\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/efdl_hw/week04_large_models/tensor_parallel_llama_cuda.py\", line 288, in <module>\n",
      "[rank0]:     assert torch.allclose(tp_input.grad, ref_input_grad, atol=1e-4), f\"input_grad mismatch on {rank=}\"\n",
      "[rank0]: AssertionError: input_grad mismatch on rank=0\n",
      "[rank0]:[W223 08:34:18.021973109 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0223 08:34:19.887000 6618 site-packages/torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 6622 closing signal SIGTERM\n",
      "E0223 08:34:20.151000 6618 site-packages/torch/distributed/elastic/multiprocessing/api.py:984] failed (exitcode: 1) local_rank: 0 (pid: 6621) of binary: /venv/main/bin/python3.10\n",
      "Traceback (most recent call last):\n",
      "  File \"/venv/main/bin/torchrun\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 362, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/distributed/run.py\", line 991, in main\n",
      "    run(args)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/distributed/run.py\", line 982, in run\n",
      "    elastic_launch(\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 170, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 317, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "tensor_parallel_llama_cuda.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2026-02-23_08:34:20\n",
      "  host      : e1bf70b6cbb5\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : -15 (pid: 6622)\n",
      "  error_file: <N/A>\n",
      "  traceback : Signal 15 (SIGTERM) received by PID 6622\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2026-02-23_08:34:19\n",
      "  host      : e1bf70b6cbb5\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 6621)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "CPU times: user 190 ms, sys: 221 ms, total: 411 ms\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!OMP_NUM_THREADS=1 torchrun --nproc_per_node 2 tensor_parallel_llama_cuda.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ð±Ð¾Ð»ÑŒÑˆÐ°Ñ Ñ‡Ð°ÑÑ‚ÑŒ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ ÑƒÑ…Ð¾Ð´Ð¸Ñ‚ Ð½Ð° Ñ‚Ð¾ Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ð°Ñ‡Ð°Ñ‚ÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWKuXKS3jsLY"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Sequence Parallelism with Ulysses\n",
    "\n",
    "\n",
    "Now let's parallelize the other way - across the sequence dimension. To showcase why this is necessary, our main task will be to parallelize LLM fine-tuning over a very long sequence. The way you do this, of course, is through Sequence Parallelism. You can implement naive [sequence parallelism](https://arxiv.org/abs/2205.05198), similar to [DeepSpeed Ulysses](https://arxiv.org/pdf/2309.14509) (n.b.: not the first work to do this).\n",
    "\n",
    "![figure-from-paper](https://ar5iv.labs.arxiv.org/html/2309.14509/assets/figs/image3.png)\n",
    "\n",
    "\n",
    "Here's the short version:\n",
    "- All weights are replicated between ranks (optionally: FSDP)\n",
    "- Each rank holds a subset of sequence tokens\n",
    "- Embeddings, logits, normalizations, MLP all apply independently to token shards\n",
    "- The multi-head attention is the only layer that gets special treatment\n",
    "    - First, apply QKV projections to local tokens, as in data-parallel training;\n",
    "    - Then re-shard so that each rank holds a **subset of heads** across **all tokens**;\n",
    "    - Compute the attention ''core'' (RoPE and F.scaled_dot_product_attention) for its chunk of heads independently;\n",
    "    - Re-shard outputs again so that each rank concatenates **all heads**, but only for its **subset of tokens**;\n",
    "    - Apply the output (\"O\") projection to your local tokens again.\n",
    "- This approach *may* be combined with tensor parallelism, but this is an advanced technique that you don't have to implement.\n",
    "\n",
    "\n",
    "__You have a choice__ between two options on how to implement it: either manually with torch.distributed like in task 2, or using the DTensor route like in task 3. We provide some tips for both tasks.\n",
    "\n",
    "\n",
    "**Option A. with raw `torch.distirbuted`:**\n",
    "- Use [`dist.all_to_all`](https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_to_all) to switch between per-token and per-head sharding without materializing the full tensor on any device;\n",
    "- Wrap the model with [`DistributedDataParallel`](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) or [`FullyShardedDataParallel`](https://pytorch.org/docs/stable/fsdp.html) so that fine-tuning synchronizes trainable parameters. Note that using FSDP for parameter-efficient fine-tuning can be tricky: we recommend you either wrap **trainable modules** with separate FSDP sub-instances via auto_wrap_policy - or simply use DDP instead of FSDP.\n",
    "\n",
    "**Option B. with `DTensor`:**\n",
    "- We recommend you first skim the official [tutorial](https://pytorch.org/tutorials/intermediate/TP_tutorial.html) on applying Tensor Parallelism (sic.) - or browse the [TorchTitan's version](https://github.com/pytorch/torchtitan/blob/82afc842e303e49d1a137fc7ea48291a57f72d5d/torchtitan/models/llama/parallelize_llama.py) of it.\n",
    "- Note that there is a [`SequenceParallel`](https://pytorch.org/docs/stable/distributed.tensor.parallel.html#torch.distributed.tensor.parallel.SequenceParallel) class in torch.distributed.tensor.parallel` - **however, it does not magick the sequence parallelism for you** - it is only meant for small layers (e.g. normalization). You still need to do the sharding in self-attention!\n",
    "\n",
    "For the sake of formality, here's an example script you need to parallelize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539,
     "referenced_widgets": [
      "bdfc71a449484eaeb3539fabb60a767c",
      "c5a57f756d474bdea5ce6b012edf1508",
      "ce7ec364252f46b996ad0b32fe9d38cd",
      "8d369b7163074ebd822d6702878256a4",
      "ab8fc563343e4e7ea09356642de10756",
      "6a9ff0b43f794891ba1f5c4ecd6dcfdf",
      "3787e2bdb8cd42a8bf1815a090ed9706",
      "b944e375f23d4ea98e58533f11857d33",
      "44d7e3d82cc54756a70a287bbd39ecb9",
      "6f4ac58e5676482e9bcb949adc4f84d1",
      "d2f335cbff4e4ff2baa02a2ad7b82a3e",
      "1aa8ab0872514d57b0592ba43ea0105a",
      "bc7ba0334162415ba5f27f20deb28247",
      "c0e6c92aaebc4f02a4943a572d8220dd",
      "db12af0c8a0d41fa981dbb66e7ed49d4",
      "4b7c453fb3994045a855b7370eee234f",
      "c6e968957e8c4a21953a2d3b9ddd4f8d",
      "2ac894dade0c48b7a30c058eded65b73",
      "94f4a642365b4e69aa15e7ea2bd1a29b",
      "fa7d727b36b742419a928e271d403bba",
      "6ee6f803e8a34061b52b09a1ed9c3a29",
      "dcd4b9306a21422abdd2073913493326",
      "d6509faf9220437e9371f83b5058fc29",
      "d8e556d9c6c04603905217901cb38ac7",
      "820a152271204ba18f6d4808e9fca90e",
      "55e4bf33a30e400db72d47570e2345f8",
      "5ca5324606e84da48635840a32e94f61",
      "6a6840053fad4785a2124aea7c261f59",
      "f1c7bf7153f14344a6e4a7a9bdc35b53",
      "b26d8fd37493485a9c69a20de99262bf",
      "76cbb91b2815478ea29d59b9e4bbc937",
      "7b69fc37e8054da1977eebf3ab652157",
      "0f03319b7e6f4fcc93e2b91ca3293584",
      "8d891195bee042c5b3c88411367196ca",
      "01dd79c9b6404d3c98affd6666ca2d22",
      "50b1790060e94fab95d79317cc0dc333",
      "f9649cfc2d54427ca3a70f87a82f4a5f",
      "57f7eea22c934910b8897d340edf3107",
      "fff708ec9f324f8980f8549639dae9ee",
      "92c3644edc2c48aca666dc94b267dda8",
      "1c80c6a948d84ab2810ea45af8d27af2",
      "74c395453ab64b0899505f152a12c50a",
      "94a2a7b26e984778a2186a491bf06b4f",
      "a7c591494d654cdd8a19141cf96d8036",
      "d77be2d56f3846d4991c92ee953731ec",
      "f940bf4b3d454ad7a3592b0cc51979fd",
      "8339128e688d4221bc318b45910404a6",
      "0309561fa3734a0ea3cd3421c420ff56",
      "2329d7fe319b4383ab87aabe5e3e17fa",
      "df6158bafdd94f4e8b9f147acc0ef8f5",
      "48669f5193124c7ca52d554a998be789",
      "a12be7a58f764dff96d41ba35d3a9c76",
      "ad6a69b6a4614050a497eccebdc920b2",
      "37919cdcd7304db2a60df1cc387ca4ce",
      "d1cbd32f079042398389dd2f250cc8ef",
      "0b353dfbb9784b3883ff31ed7124f450",
      "443a315962ab46ea8b6d8f2ed354038e",
      "d1d68bb4f42647e39806f1fc4dd9c014",
      "ede69839385f48399d51b0c9a0568353",
      "5768b9b7ffec4884869b2334eb946015",
      "9045df041cac425fa7e109d303bb4d2f",
      "0dc89a0bd4e34ce1b60597cb1acd7fdf",
      "5a5c1dcd5a214c93bb917c8eb9029af2",
      "c5f9ad4f3dd94e8b8d05530f11fed714",
      "fd64196b94f64bf59d4bbd1307aaf05c",
      "d643e91490274b1988b242e4d784ff82"
     ]
    },
    "id": "9ofi1_Kgusd8",
    "outputId": "d456bd60-be30-4f81-917d-5ef3d9e04538"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfc71a449484eaeb3539fabb60a767c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa8ab0872514d57b0592ba43ea0105a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6509faf9220437e9371f83b5058fc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d891195bee042c5b3c88411367196ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77be2d56f3846d4991c92ee953731ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b353dfbb9784b3883ff31ed7124f450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (397368 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping input_ids.shape[1]=397368 to 128 tokens\n",
      "Parameters: 65536 trainable / 1235879936 total\n",
      "i=0\tloss.item()=8.582364082336426\n",
      "i=1\tloss.item()=8.413138389587402\n",
      "i=2\tloss.item()=8.251019477844238\n",
      "i=3\tloss.item()=8.113922119140625\n",
      "i=4\tloss.item()=8.001195907592773\n",
      "i=5\tloss.item()=7.875072002410889\n",
      "i=6\tloss.item()=7.784201145172119\n",
      "i=7\tloss.item()=7.691455364227295\n",
      "i=8\tloss.item()=7.621612548828125\n",
      "i=9\tloss.item()=7.547983169555664\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B\"  # for testing (but not grading!), you may want to use Maykeye/TinyLLama-v0\n",
    "SEQUENCE_LENGTH = 128                # IMPORTANT!!! you need to increase this parameter! Look for the maximum sequence length on one and multiple GPUs\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.required_grad = False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "model = peft.get_peft_model(model, peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=32))\n",
    "assert any(param.requires_grad for param in model.parameters()), \"No trainable parameters - did you enable PEFT?\"\n",
    "\n",
    "!wget -q https://www.gutenberg.org/cache/epub/4300/pg4300.txt -O ulysses.txt  # ... or use any other text of your choosing\n",
    "input_ids = tokenizer(open(\"ulysses.txt\").read(), return_tensors='pt')['input_ids']\n",
    "print(f\"Cropping {input_ids.shape[1]=} to {SEQUENCE_LENGTH} tokens\")\n",
    "input_ids, labels = input_ids[:, :SEQUENCE_LENGTH], input_ids[:, 1:SEQUENCE_LENGTH + 1]\n",
    "\n",
    "trainable_parameters = {p for p in model.parameters() if p.requires_grad}\n",
    "print(f\"Parameters: {sum(map(torch.Tensor.numel, trainable_parameters))} trainable / {sum(map(torch.Tensor.numel, model.parameters()))} total\")\n",
    "opt = torch.optim.Adam(trainable_parameters)\n",
    "for i in range(10):\n",
    "  loss = model(input_ids=input_ids.to(device), labels=labels.to(device)).loss\n",
    "  opt.zero_grad()\n",
    "  loss.backward()\n",
    "  opt.step()\n",
    "  print(f\"{i=}\\t{loss.item()=}\")\n",
    "\n",
    "# pro tip: delete the model or restart session to free RAM for the TP experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kelsoIRmtyPL"
   },
   "source": [
    "__Task 4 (1 point):__ before you do training, let's first parallelize a single forward pass. Implement sharding with the same interface you used in tasks 2 (or 3 if you use DTensor), but this time, parallelize across the sequence dimension. Note: if you are running out of (V)RAM, load the 1B model in half precision and disable gradients for all weights except the first (few) layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xwwjXfPdnqt"
   },
   "outputs": [],
   "source": [
    "<<A whole lot of your code here>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7Vw7iH-kLMc"
   },
   "outputs": [],
   "source": [
    "<<... and a dedicated cell to show off that it works>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oS4JWk3J7oP9"
   },
   "source": [
    "__Task 5 (1 point):__ Now use the script above to parallelize the entire training run. You are free to use other fine-tuning methods (e.g. LoRA or even full fine-tuning), as long as you can demonstrate that the loss goes down.\n",
    "\n",
    "**Make sure you increase SEQUENCE_LENGTH as much as possible!** Even on a single GPU, you should be able to go into thousands, if not tens of thousands of tokens - and report the maximum sequence length with one and with multiple GPUs respectively.\n",
    "\n",
    "If you don't have access to multiple GPUs, you may optionally submit a version that does training on a single GPU, but computes attention heads sequentially with gradient checkpointing - but if you do, please announce that you are using this option in bold, capital letters, so that the grader will notice it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azcZBvaa8ymE"
   },
   "outputs": [],
   "source": [
    "<<A whole lot of your code here>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBWi66FkC1dz"
   },
   "outputs": [],
   "source": [
    "<<... and a dedicated cell to show off that it works>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZFeok82812C"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Optional: bonus tasks\n",
    "\n",
    "There are many routes to further improve the training/inference code. You may (but you don't have to) implement any combination of them for bonus points.\n",
    "\n",
    "However, please not that the total points for this week's entire assignment (part 1 & 2) are **capped at 14**.\n",
    "\n",
    "__Bonus task: parallel key-value caching (1 point).__ In tasks 2 and 3, you implement tensor parallelism for attention forward pass and perform inference with re-computation. However, real world inference engines use [KV caching](https://huggingface.co/docs/transformers/main/en/kv_cache) - keeping key and value caches from past tokens and only processing the new token each time.\n",
    "\n",
    "For this task, you will have to implement this type of parallelism for either torch.distributed or DTensor implementation of attention $-$ simply cache the heads already assigned to each rank. To get the grade, you will need to demonstrate that the model generates a sensible text with any cache (via past_key_values=).\n",
    "\n",
    "__Bonus task: pipeline parallelism (1-2 points):__ In tasks 1-3, you've implemented symmetric model parallelism, aka Tensor Parallelism. However, there is another way to partition model parameters $-$ assign entire layers to each rank and run them in a pipeline. This can be faster, especially if you are running\n",
    "\n",
    "For 1 point, check out [torch.distributed.pipelinging](https://pytorch.org/docs/stable/distributed.pipelining.html), [DeepSpeed pipelining](https://deepspeed.readthedocs.io/en/latest/pipeline.html) or [torchgpipe](https://github.com/kakaobrain/torchgpipe) and demonstrate that you can run or fine-tune a model that would not fit into a single GPU (you will need multuple devices for this!).\n",
    "\n",
    "For 2 points, compare different pipelining schedules in terms of training throughput: use GPipe as a baseline and try ScheduleInterleaved1F1B (or a more advanced pipeline of your choosing).\n",
    "\n",
    "__Bonus task: better sequence parallelism (2 points).__ In tasks 4 and 5, you implemented basic sequence parallelism. However, there are multiple ways you can improve that technique for further memory savings or better device utilization.\n",
    "\n",
    "For 1 point, implement combined tensor + sequence parallelism and compare results with naive sequence parallelism.\n",
    "\n",
    "For 2 points, implement [Ring Attention](https://arxiv.org/abs/2310.01889) *or* integrate computation-communication overlap from [FLUX](https://arxiv.org/abs/2406.06858) and measure the speed and memory trade-offs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01dd79c9b6404d3c98affd6666ca2d22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fff708ec9f324f8980f8549639dae9ee",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_92c3644edc2c48aca666dc94b267dda8",
      "value": "config.json:â€‡100%"
     }
    },
    "0309561fa3734a0ea3cd3421c420ff56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37919cdcd7304db2a60df1cc387ca4ce",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d1cbd32f079042398389dd2f250cc8ef",
      "value": "â€‡2.47G/2.47Gâ€‡[00:59&lt;00:00,â€‡42.7MB/s]"
     }
    },
    "0b353dfbb9784b3883ff31ed7124f450": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_443a315962ab46ea8b6d8f2ed354038e",
       "IPY_MODEL_d1d68bb4f42647e39806f1fc4dd9c014",
       "IPY_MODEL_ede69839385f48399d51b0c9a0568353"
      ],
      "layout": "IPY_MODEL_5768b9b7ffec4884869b2334eb946015"
     }
    },
    "0dc89a0bd4e34ce1b60597cb1acd7fdf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f03319b7e6f4fcc93e2b91ca3293584": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1aa8ab0872514d57b0592ba43ea0105a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bc7ba0334162415ba5f27f20deb28247",
       "IPY_MODEL_c0e6c92aaebc4f02a4943a572d8220dd",
       "IPY_MODEL_db12af0c8a0d41fa981dbb66e7ed49d4"
      ],
      "layout": "IPY_MODEL_4b7c453fb3994045a855b7370eee234f"
     }
    },
    "1c80c6a948d84ab2810ea45af8d27af2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2329d7fe319b4383ab87aabe5e3e17fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ac894dade0c48b7a30c058eded65b73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3787e2bdb8cd42a8bf1815a090ed9706": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37919cdcd7304db2a60df1cc387ca4ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "443a315962ab46ea8b6d8f2ed354038e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9045df041cac425fa7e109d303bb4d2f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0dc89a0bd4e34ce1b60597cb1acd7fdf",
      "value": "generation_config.json:â€‡100%"
     }
    },
    "44d7e3d82cc54756a70a287bbd39ecb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48669f5193124c7ca52d554a998be789": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b7c453fb3994045a855b7370eee234f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50b1790060e94fab95d79317cc0dc333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c80c6a948d84ab2810ea45af8d27af2",
      "max": 935,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74c395453ab64b0899505f152a12c50a",
      "value": 935
     }
    },
    "55e4bf33a30e400db72d47570e2345f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b69fc37e8054da1977eebf3ab652157",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0f03319b7e6f4fcc93e2b91ca3293584",
      "value": "â€‡459/459â€‡[00:00&lt;00:00,â€‡14.2kB/s]"
     }
    },
    "5768b9b7ffec4884869b2334eb946015": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57f7eea22c934910b8897d340edf3107": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a5c1dcd5a214c93bb917c8eb9029af2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ca5324606e84da48635840a32e94f61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a6840053fad4785a2124aea7c261f59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a9ff0b43f794891ba1f5c4ecd6dcfdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ee6f803e8a34061b52b09a1ed9c3a29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f4ac58e5676482e9bcb949adc4f84d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74c395453ab64b0899505f152a12c50a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "76cbb91b2815478ea29d59b9e4bbc937": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7b69fc37e8054da1977eebf3ab652157": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "820a152271204ba18f6d4808e9fca90e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b26d8fd37493485a9c69a20de99262bf",
      "max": 459,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76cbb91b2815478ea29d59b9e4bbc937",
      "value": 459
     }
    },
    "8339128e688d4221bc318b45910404a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a12be7a58f764dff96d41ba35d3a9c76",
      "max": 2471645608,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ad6a69b6a4614050a497eccebdc920b2",
      "value": 2471645608
     }
    },
    "8d369b7163074ebd822d6702878256a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f4ac58e5676482e9bcb949adc4f84d1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d2f335cbff4e4ff2baa02a2ad7b82a3e",
      "value": "â€‡50.6k/50.6kâ€‡[00:00&lt;00:00,â€‡2.96MB/s]"
     }
    },
    "8d891195bee042c5b3c88411367196ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_01dd79c9b6404d3c98affd6666ca2d22",
       "IPY_MODEL_50b1790060e94fab95d79317cc0dc333",
       "IPY_MODEL_f9649cfc2d54427ca3a70f87a82f4a5f"
      ],
      "layout": "IPY_MODEL_57f7eea22c934910b8897d340edf3107"
     }
    },
    "9045df041cac425fa7e109d303bb4d2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92c3644edc2c48aca666dc94b267dda8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94a2a7b26e984778a2186a491bf06b4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94f4a642365b4e69aa15e7ea2bd1a29b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a12be7a58f764dff96d41ba35d3a9c76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7c591494d654cdd8a19141cf96d8036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab8fc563343e4e7ea09356642de10756": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad6a69b6a4614050a497eccebdc920b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b26d8fd37493485a9c69a20de99262bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b944e375f23d4ea98e58533f11857d33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc7ba0334162415ba5f27f20deb28247": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6e968957e8c4a21953a2d3b9ddd4f8d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2ac894dade0c48b7a30c058eded65b73",
      "value": "tokenizer.json:â€‡100%"
     }
    },
    "bdfc71a449484eaeb3539fabb60a767c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5a57f756d474bdea5ce6b012edf1508",
       "IPY_MODEL_ce7ec364252f46b996ad0b32fe9d38cd",
       "IPY_MODEL_8d369b7163074ebd822d6702878256a4"
      ],
      "layout": "IPY_MODEL_ab8fc563343e4e7ea09356642de10756"
     }
    },
    "c0e6c92aaebc4f02a4943a572d8220dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94f4a642365b4e69aa15e7ea2bd1a29b",
      "max": 17209920,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fa7d727b36b742419a928e271d403bba",
      "value": 17209920
     }
    },
    "c5a57f756d474bdea5ce6b012edf1508": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a9ff0b43f794891ba1f5c4ecd6dcfdf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3787e2bdb8cd42a8bf1815a090ed9706",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "c5f9ad4f3dd94e8b8d05530f11fed714": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c6e968957e8c4a21953a2d3b9ddd4f8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce7ec364252f46b996ad0b32fe9d38cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b944e375f23d4ea98e58533f11857d33",
      "max": 50646,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_44d7e3d82cc54756a70a287bbd39ecb9",
      "value": 50646
     }
    },
    "d1cbd32f079042398389dd2f250cc8ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1d68bb4f42647e39806f1fc4dd9c014": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a5c1dcd5a214c93bb917c8eb9029af2",
      "max": 230,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5f9ad4f3dd94e8b8d05530f11fed714",
      "value": 230
     }
    },
    "d2f335cbff4e4ff2baa02a2ad7b82a3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d643e91490274b1988b242e4d784ff82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6509faf9220437e9371f83b5058fc29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8e556d9c6c04603905217901cb38ac7",
       "IPY_MODEL_820a152271204ba18f6d4808e9fca90e",
       "IPY_MODEL_55e4bf33a30e400db72d47570e2345f8"
      ],
      "layout": "IPY_MODEL_5ca5324606e84da48635840a32e94f61"
     }
    },
    "d77be2d56f3846d4991c92ee953731ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f940bf4b3d454ad7a3592b0cc51979fd",
       "IPY_MODEL_8339128e688d4221bc318b45910404a6",
       "IPY_MODEL_0309561fa3734a0ea3cd3421c420ff56"
      ],
      "layout": "IPY_MODEL_2329d7fe319b4383ab87aabe5e3e17fa"
     }
    },
    "d8e556d9c6c04603905217901cb38ac7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a6840053fad4785a2124aea7c261f59",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f1c7bf7153f14344a6e4a7a9bdc35b53",
      "value": "special_tokens_map.json:â€‡100%"
     }
    },
    "db12af0c8a0d41fa981dbb66e7ed49d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ee6f803e8a34061b52b09a1ed9c3a29",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_dcd4b9306a21422abdd2073913493326",
      "value": "â€‡17.2M/17.2Mâ€‡[00:00&lt;00:00,â€‡36.3MB/s]"
     }
    },
    "dcd4b9306a21422abdd2073913493326": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df6158bafdd94f4e8b9f147acc0ef8f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ede69839385f48399d51b0c9a0568353": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd64196b94f64bf59d4bbd1307aaf05c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d643e91490274b1988b242e4d784ff82",
      "value": "â€‡230/230â€‡[00:00&lt;00:00,â€‡15.4kB/s]"
     }
    },
    "f1c7bf7153f14344a6e4a7a9bdc35b53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f940bf4b3d454ad7a3592b0cc51979fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df6158bafdd94f4e8b9f147acc0ef8f5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_48669f5193124c7ca52d554a998be789",
      "value": "model.safetensors:â€‡100%"
     }
    },
    "f9649cfc2d54427ca3a70f87a82f4a5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94a2a7b26e984778a2186a491bf06b4f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a7c591494d654cdd8a19141cf96d8036",
      "value": "â€‡935/935â€‡[00:00&lt;00:00,â€‡56.1kB/s]"
     }
    },
    "fa7d727b36b742419a928e271d403bba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fd64196b94f64bf59d4bbd1307aaf05c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fff708ec9f324f8980f8549639dae9ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
