{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f77e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                     [100%]\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________________ test_fsdp_with_itself[float32-bfloat16] ____________________\u001b[0m\n",
      "\n",
      "param_dtype = 'bfloat16', reduce_dtype = 'float32'\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparam_dtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mNone\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mbfloat16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mreduce_dtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mbfloat16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_fsdp_with_itself\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        param_dtype: Literal[\u001b[33m\"\u001b[39;49;00m\u001b[33mbfloat16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] | \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        reduce_dtype: Literal[\u001b[33m\"\u001b[39;49;00m\u001b[33mbfloat16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] | \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      ">       run_distributed_test(\u001b[90m\u001b[39;49;00m\n",
      "            _test_fsdp_with_itself,\u001b[90m\u001b[39;49;00m\n",
      "            param_dtype=param_dtype,\u001b[90m\u001b[39;49;00m\n",
      "            reduce_dtype=reduce_dtype,\u001b[90m\u001b[39;49;00m\n",
      "            world_size=\u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtest.py\u001b[0m:116: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtest.py\u001b[0m:82: in run_distributed_test\n",
      "    \u001b[0mmp.start_processes(  \u001b[90m# type: ignore[attr-defined,no-untyped-call]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m.venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\u001b[0m:296: in start_processes\n",
      "    \u001b[0m\u001b[94mwhile\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m context.join():\u001b[90m\u001b[39;49;00m\n",
      "              ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f74a68a0d10>\n",
      "timeout = None, grace_period = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mjoin\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, timeout: \u001b[96mfloat\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m, grace_period: \u001b[96mfloat\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\"\"Join one or more processes within spawn context.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Attempt to join one or more processes in this spawn context.\u001b[39;49;00m\n",
      "    \u001b[33m    If one of them exited with a non-zero exit status, this function\u001b[39;49;00m\n",
      "    \u001b[33m    kills the remaining processes (optionally with a grace period)\u001b[39;49;00m\n",
      "    \u001b[33m    and raises an exception with the cause of the first process exiting.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns ``True`` if all processes have been joined successfully,\u001b[39;49;00m\n",
      "    \u001b[33m    ``False`` if there are more processes that need to be joined.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        timeout (float): Wait this long (in seconds) before giving up on waiting.\u001b[39;49;00m\n",
      "    \u001b[33m        grace_period (float): When any processes fail, wait this long (in seconds)\u001b[39;49;00m\n",
      "    \u001b[33m            for others to shutdown gracefully before terminating them. If they\u001b[39;49;00m\n",
      "    \u001b[33m            still don't exit, wait another grace period before killing them.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Ensure this function can be called even when we're done.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.sentinels) == \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Wait for any process to fail or all of them to succeed.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ready = multiprocessing.connection.wait(\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.sentinels.keys(),\u001b[90m\u001b[39;49;00m\n",
      "            timeout=timeout,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        error_index = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m sentinel \u001b[95min\u001b[39;49;00m ready:\u001b[90m\u001b[39;49;00m\n",
      "            index = \u001b[96mself\u001b[39;49;00m.sentinels.pop(sentinel)\u001b[90m\u001b[39;49;00m\n",
      "            process = \u001b[96mself\u001b[39;49;00m.processes[index]\u001b[90m\u001b[39;49;00m\n",
      "            process.join()\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m process.exitcode != \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                error_index = index\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Return if there was no error.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m error_index \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Return whether or not all processes have been joined.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.sentinels) == \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# An error occurred. Clean-up all processes before returning.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# First, allow a grace period for processes to shutdown themselves.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m grace_period \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m._join_procs_with_timeout(grace_period)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Then, terminate processes that are still alive. Try SIGTERM first.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m process \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.processes:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m process.is_alive():\u001b[90m\u001b[39;49;00m\n",
      "                log.warning(\u001b[33m\"\u001b[39;49;00m\u001b[33mTerminating process \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m via signal SIGTERM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, process.pid)\u001b[90m\u001b[39;49;00m\n",
      "                process.terminate()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Try SIGKILL if the process isn't going down after another grace_period.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# The reason is related to python signal handling is limited\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# to main thread and if that is in c/c++ land and stuck it won't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# to handle it. We have seen processes getting stuck not handling\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# SIGTERM for the above reason.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m._join_procs_with_timeout(\u001b[94m30\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m grace_period \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m grace_period)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m process \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.processes:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m process.is_alive():\u001b[90m\u001b[39;49;00m\n",
      "                log.warning(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mUnable to shutdown process \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m via SIGTERM , forcefully exiting via SIGKILL\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    process.pid,\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "                process.kill()\u001b[90m\u001b[39;49;00m\n",
      "            process.join()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# The file will only be created if the process crashed.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        failed_process = \u001b[96mself\u001b[39;49;00m.processes[error_index]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m os.access(\u001b[96mself\u001b[39;49;00m.error_files[error_index], os.R_OK):\u001b[90m\u001b[39;49;00m\n",
      "            exitcode = \u001b[96mself\u001b[39;49;00m.processes[error_index].exitcode\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m exitcode < \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    name = signal.Signals(-exitcode).name\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mexcept\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    name = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m<Unknown signal \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m-exitcode\u001b[33m}\u001b[39;49;00m\u001b[33m>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m ProcessExitedException(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mprocess \u001b[39;49;00m\u001b[33m{\u001b[39;49;00merror_index\u001b[33m:\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m terminated with signal \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mname\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    error_index=error_index,\u001b[90m\u001b[39;49;00m\n",
      "                    error_pid=failed_process.pid,\u001b[90m\u001b[39;49;00m\n",
      "                    exit_code=exitcode,\u001b[90m\u001b[39;49;00m\n",
      "                    signal_name=name,\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m ProcessExitedException(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mprocess \u001b[39;49;00m\u001b[33m{\u001b[39;49;00merror_index\u001b[33m:\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m terminated with exit code \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mexitcode\u001b[33m:\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    error_index=error_index,\u001b[90m\u001b[39;49;00m\n",
      "                    error_pid=failed_process.pid,\u001b[90m\u001b[39;49;00m\n",
      "                    exit_code=exitcode,\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.error_files[error_index], \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m fh:\u001b[90m\u001b[39;49;00m\n",
      "            original_trace = pickle.load(fh)\u001b[90m\u001b[39;49;00m\n",
      "        msg = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m-- Process \u001b[39;49;00m\u001b[33m{\u001b[39;49;00merror_index\u001b[33m:\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m terminated with the following error:\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        msg += original_trace\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m ProcessRaisedException(msg, error_index, failed_process.pid)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       torch.multiprocessing.spawn.ProcessRaisedException: \u001b[0m\n",
      "\u001b[1m\u001b[31mE       \u001b[0m\n",
      "\u001b[1m\u001b[31mE       -- Process 1 terminated with the following error:\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Traceback (most recent call last):\u001b[0m\n",
      "\u001b[1m\u001b[31mE         File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 87, in _wrap\u001b[0m\n",
      "\u001b[1m\u001b[31mE           fn(i, *args)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         File \"/workspace/efdl_hw/week05_fsdp/homework/test.py\", line 69, in _run_test\u001b[0m\n",
      "\u001b[1m\u001b[31mE           func(*func_args, **func_kwargs)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         File \"/workspace/efdl_hw/week05_fsdp/homework/test.py\", line 164, in _test_fsdp_with_itself\u001b[0m\n",
      "\u001b[1m\u001b[31mE           assert fsdp_loss == effdl_loss\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  ^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m.venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\u001b[0m:211: ProcessRaisedException\n",
      "----------------------------- Captured stderr call -----------------------------\n",
      "2026-02-26 22:12:30,794 - train - INFO - model size: 6163712 parameters.\n",
      "2026-02-26 22:12:30,803 - train - INFO - model size: 6163712 parameters.\n",
      "2026-02-26 22:12:30,827 - root - INFO - Loading tokenizer from tokenizer.json\n",
      "2026-02-26 22:12:30,829 - root - INFO - Preparing c4_test dataset from ./c4_test\n",
      "2026-02-26 22:12:30,839 - root - INFO - Loading tokenizer from tokenizer.json\n",
      "2026-02-26 22:12:30,842 - root - INFO - Preparing c4_test dataset from ./c4_test\n",
      "2026-02-26 22:12:32,583 - train - INFO - step:  1  loss:  7.7014  grad_norm:  2.2645\n",
      "2026-02-26 22:12:32,591 - train - INFO - step:  1  loss:  7.7014  grad_norm:  2.2645\n",
      "2026-02-26 22:12:32,633 - train - INFO - step:  2  loss:  6.3579  grad_norm:  2.8387\n",
      "2026-02-26 22:12:32,634 - train - INFO - step:  2  loss:  6.3579  grad_norm:  2.8387\n",
      "2026-02-26 22:12:32,674 - train - INFO - step:  3  loss:  5.8355  grad_norm:  2.8505\n",
      "2026-02-26 22:12:32,676 - train - INFO - step:  3  loss:  5.8355  grad_norm:  2.8505\n",
      "2026-02-26 22:12:32,715 - train - INFO - step:  4  loss:  5.2045  grad_norm:  3.1455\n",
      "2026-02-26 22:12:32,718 - train - INFO - step:  4  loss:  5.2045  grad_norm:  3.1455\n",
      "2026-02-26 22:12:32,758 - train - INFO - step:  5  loss:  4.8259  grad_norm:  2.8325\n",
      "2026-02-26 22:12:32,759 - train - INFO - step:  5  loss:  4.8259  grad_norm:  2.8325\n",
      "2026-02-26 22:12:32,766 - train - INFO - model size: 6163712 parameters.\n",
      "2026-02-26 22:12:32,767 - train - INFO - model size: 6163712 parameters.\n",
      "2026-02-26 22:12:32,783 - root - INFO - Loading tokenizer from tokenizer.json\n",
      "2026-02-26 22:12:32,785 - root - INFO - Preparing c4_test dataset from ./c4_test\n",
      "2026-02-26 22:12:32,785 - root - INFO - Loading tokenizer from tokenizer.json\n",
      "2026-02-26 22:12:32,786 - root - INFO - Preparing c4_test dataset from ./c4_test\n",
      "2026-02-26 22:12:32,855 - train - INFO - step:  1  loss:  7.7014  grad_norm:  2.2645\n",
      "2026-02-26 22:12:32,857 - train - INFO - step:  1  loss:  7.7014  grad_norm:  2.2645\n",
      "2026-02-26 22:12:32,900 - train - INFO - step:  2  loss:  6.3584  grad_norm:  2.8386\n",
      "2026-02-26 22:12:32,901 - train - INFO - step:  2  loss:  6.3584  grad_norm:  2.8386\n",
      "2026-02-26 22:12:32,947 - train - INFO - step:  3  loss:  5.8351  grad_norm:  2.8503\n",
      "2026-02-26 22:12:32,948 - train - INFO - step:  3  loss:  5.8351  grad_norm:  2.8503\n",
      "2026-02-26 22:12:32,994 - train - INFO - step:  4  loss:  5.2044  grad_norm:  3.1454\n",
      "2026-02-26 22:12:32,994 - train - INFO - step:  4  loss:  5.2044  grad_norm:  3.1454\n",
      "2026-02-26 22:12:33,041 - train - INFO - step:  5  loss:  4.8257  grad_norm:  2.8324\n",
      "2026-02-26 22:12:33,042 - train - INFO - step:  5  loss:  4.8257  grad_norm:  2.8324\n",
      "W0226 22:12:34.499000 81447 .venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:165] Terminating process 81727 via signal SIGTERM\n",
      "\u001b[31m\u001b[1m___________________ test_fsdp_with_itself[bfloat16-bfloat16] ___________________\u001b[0m\n",
      "\n",
      "param_dtype = 'bfloat16', reduce_dtype = 'bfloat16'\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparam_dtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mNone\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mbfloat16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mreduce_dtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mbfloat16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_fsdp_with_itself\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        param_dtype: Literal[\u001b[33m\"\u001b[39;49;00m\u001b[33mbfloat16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] | \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        reduce_dtype: Literal[\u001b[33m\"\u001b[39;49;00m\u001b[33mbfloat16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] | \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ):\u001b[90m\u001b[39;49;00m\n",
      ">       run_distributed_test(\u001b[90m\u001b[39;49;00m\n",
      "            _test_fsdp_with_itself,\u001b[90m\u001b[39;49;00m\n",
      "            param_dtype=param_dtype,\u001b[90m\u001b[39;49;00m\n",
      "            reduce_dtype=reduce_dtype,\u001b[90m\u001b[39;49;00m\n",
      "            world_size=\u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtest.py\u001b[0m:116: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtest.py\u001b[0m:82: in run_distributed_test\n",
      "    \u001b[0mmp.start_processes(  \u001b[90m# type: ignore[attr-defined,no-untyped-call]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m.venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\u001b[0m:296: in start_processes\n",
      "    \u001b[0m\u001b[94mwhile\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m context.join():\u001b[90m\u001b[39;49;00m\n",
      "              ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f74a6445d30>\n",
      "timeout = None, grace_period = None\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mjoin\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, timeout: \u001b[96mfloat\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m, grace_period: \u001b[96mfloat\u001b[39;49;00m | \u001b[94mNone\u001b[39;49;00m = \u001b[94mNone\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\"\"Join one or more processes within spawn context.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Attempt to join one or more processes in this spawn context.\u001b[39;49;00m\n",
      "    \u001b[33m    If one of them exited with a non-zero exit status, this function\u001b[39;49;00m\n",
      "    \u001b[33m    kills the remaining processes (optionally with a grace period)\u001b[39;49;00m\n",
      "    \u001b[33m    and raises an exception with the cause of the first process exiting.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns ``True`` if all processes have been joined successfully,\u001b[39;49;00m\n",
      "    \u001b[33m    ``False`` if there are more processes that need to be joined.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        timeout (float): Wait this long (in seconds) before giving up on waiting.\u001b[39;49;00m\n",
      "    \u001b[33m        grace_period (float): When any processes fail, wait this long (in seconds)\u001b[39;49;00m\n",
      "    \u001b[33m            for others to shutdown gracefully before terminating them. If they\u001b[39;49;00m\n",
      "    \u001b[33m            still don't exit, wait another grace period before killing them.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Ensure this function can be called even when we're done.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.sentinels) == \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Wait for any process to fail or all of them to succeed.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ready = multiprocessing.connection.wait(\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.sentinels.keys(),\u001b[90m\u001b[39;49;00m\n",
      "            timeout=timeout,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        error_index = \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m sentinel \u001b[95min\u001b[39;49;00m ready:\u001b[90m\u001b[39;49;00m\n",
      "            index = \u001b[96mself\u001b[39;49;00m.sentinels.pop(sentinel)\u001b[90m\u001b[39;49;00m\n",
      "            process = \u001b[96mself\u001b[39;49;00m.processes[index]\u001b[90m\u001b[39;49;00m\n",
      "            process.join()\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m process.exitcode != \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                error_index = index\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mbreak\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Return if there was no error.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m error_index \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# Return whether or not all processes have been joined.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.sentinels) == \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# An error occurred. Clean-up all processes before returning.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# First, allow a grace period for processes to shutdown themselves.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m grace_period \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m._join_procs_with_timeout(grace_period)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Then, terminate processes that are still alive. Try SIGTERM first.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m process \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.processes:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m process.is_alive():\u001b[90m\u001b[39;49;00m\n",
      "                log.warning(\u001b[33m\"\u001b[39;49;00m\u001b[33mTerminating process \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m via signal SIGTERM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, process.pid)\u001b[90m\u001b[39;49;00m\n",
      "                process.terminate()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Try SIGKILL if the process isn't going down after another grace_period.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# The reason is related to python signal handling is limited\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# to main thread and if that is in c/c++ land and stuck it won't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# to handle it. We have seen processes getting stuck not handling\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# SIGTERM for the above reason.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m._join_procs_with_timeout(\u001b[94m30\u001b[39;49;00m \u001b[94mif\u001b[39;49;00m grace_period \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m grace_period)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m process \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.processes:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m process.is_alive():\u001b[90m\u001b[39;49;00m\n",
      "                log.warning(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mUnable to shutdown process \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m via SIGTERM , forcefully exiting via SIGKILL\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    process.pid,\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "                process.kill()\u001b[90m\u001b[39;49;00m\n",
      "            process.join()\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# The file will only be created if the process crashed.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        failed_process = \u001b[96mself\u001b[39;49;00m.processes[error_index]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m os.access(\u001b[96mself\u001b[39;49;00m.error_files[error_index], os.R_OK):\u001b[90m\u001b[39;49;00m\n",
      "            exitcode = \u001b[96mself\u001b[39;49;00m.processes[error_index].exitcode\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m exitcode < \u001b[94m0\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    name = signal.Signals(-exitcode).name\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mexcept\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    name = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m<Unknown signal \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m-exitcode\u001b[33m}\u001b[39;49;00m\u001b[33m>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m ProcessExitedException(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mprocess \u001b[39;49;00m\u001b[33m{\u001b[39;49;00merror_index\u001b[33m:\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m terminated with signal \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mname\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    error_index=error_index,\u001b[90m\u001b[39;49;00m\n",
      "                    error_pid=failed_process.pid,\u001b[90m\u001b[39;49;00m\n",
      "                    exit_code=exitcode,\u001b[90m\u001b[39;49;00m\n",
      "                    signal_name=name,\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mraise\u001b[39;49;00m ProcessExitedException(\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mprocess \u001b[39;49;00m\u001b[33m{\u001b[39;49;00merror_index\u001b[33m:\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m terminated with exit code \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mexitcode\u001b[33m:\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    error_index=error_index,\u001b[90m\u001b[39;49;00m\n",
      "                    error_pid=failed_process.pid,\u001b[90m\u001b[39;49;00m\n",
      "                    exit_code=exitcode,\u001b[90m\u001b[39;49;00m\n",
      "                )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m \u001b[96mopen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.error_files[error_index], \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[94mas\u001b[39;49;00m fh:\u001b[90m\u001b[39;49;00m\n",
      "            original_trace = pickle.load(fh)\u001b[90m\u001b[39;49;00m\n",
      "        msg = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m-- Process \u001b[39;49;00m\u001b[33m{\u001b[39;49;00merror_index\u001b[33m:\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m terminated with the following error:\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        msg += original_trace\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m ProcessRaisedException(msg, error_index, failed_process.pid)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       torch.multiprocessing.spawn.ProcessRaisedException: \u001b[0m\n",
      "\u001b[1m\u001b[31mE       \u001b[0m\n",
      "\u001b[1m\u001b[31mE       -- Process 0 terminated with the following error:\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Traceback (most recent call last):\u001b[0m\n",
      "\u001b[1m\u001b[31mE         File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 87, in _wrap\u001b[0m\n",
      "\u001b[1m\u001b[31mE           fn(i, *args)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         File \"/workspace/efdl_hw/week05_fsdp/homework/test.py\", line 69, in _run_test\u001b[0m\n",
      "\u001b[1m\u001b[31mE           func(*func_args, **func_kwargs)\u001b[0m\n",
      "\u001b[1m\u001b[31mE         File \"/workspace/efdl_hw/week05_fsdp/homework/test.py\", line 164, in _test_fsdp_with_itself\u001b[0m\n",
      "\u001b[1m\u001b[31mE           assert fsdp_loss == effdl_loss\u001b[0m\n",
      "\u001b[1m\u001b[31mE                  ^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1m\u001b[31mE       AssertionError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m.venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\u001b[0m:211: ProcessRaisedException\n",
      "----------------------------- Captured stderr call -----------------------------\n",
      "2026-02-26 22:12:49,279 - train - INFO - model size: 6163712 parameters.\n",
      "2026-02-26 22:12:49,287 - train - INFO - model size: 6163712 parameters.\n",
      "2026-02-26 22:12:49,315 - root - INFO - Loading tokenizer from tokenizer.json\n",
      "2026-02-26 22:12:49,317 - root - INFO - Preparing c4_test dataset from ./c4_test\n",
      "2026-02-26 22:12:49,320 - root - INFO - Loading tokenizer from tokenizer.json\n",
      "2026-02-26 22:12:49,322 - root - INFO - Preparing c4_test dataset from ./c4_test\n",
      "2026-02-26 22:12:51,182 - train - INFO - step:  1  loss:  7.7014  grad_norm:  2.2645\n",
      "2026-02-26 22:12:51,199 - train - INFO - step:  1  loss:  7.7014  grad_norm:  2.2645\n",
      "2026-02-26 22:12:51,243 - train - INFO - step:  2  loss:  6.3580  grad_norm:  2.8387\n",
      "2026-02-26 22:12:51,244 - train - INFO - step:  2  loss:  6.3580  grad_norm:  2.8387\n",
      "2026-02-26 22:12:51,285 - train - INFO - step:  3  loss:  5.8353  grad_norm:  2.8503\n",
      "2026-02-26 22:12:51,287 - train - INFO - step:  3  loss:  5.8353  grad_norm:  2.8503\n",
      "2026-02-26 22:12:51,327 - train - INFO - step:  4  loss:  5.2044  grad_norm:  3.1454\n",
      "2026-02-26 22:12:51,330 - train - INFO - step:  4  loss:  5.2044  grad_norm:  3.1454\n",
      "2026-02-26 22:12:51,374 - train - INFO - step:  5  loss:  4.8257  grad_norm:  2.8328\n",
      "2026-02-26 22:12:51,377 - train - INFO - step:  5  loss:  4.8257  grad_norm:  2.8328\n",
      "2026-02-26 22:12:51,382 - train - INFO - model size: 6163712 parameters.\n",
      "2026-02-26 22:12:51,386 - train - INFO - model size: 6163712 parameters.\n",
      "2026-02-26 22:12:51,400 - root - INFO - Loading tokenizer from tokenizer.json\n",
      "2026-02-26 22:12:51,402 - root - INFO - Preparing c4_test dataset from ./c4_test\n",
      "2026-02-26 22:12:51,406 - root - INFO - Loading tokenizer from tokenizer.json\n",
      "2026-02-26 22:12:51,408 - root - INFO - Preparing c4_test dataset from ./c4_test\n",
      "2026-02-26 22:12:51,480 - train - INFO - step:  1  loss:  7.7014  grad_norm:  2.2645\n",
      "2026-02-26 22:12:51,484 - train - INFO - step:  1  loss:  7.7014  grad_norm:  2.2645\n",
      "2026-02-26 22:12:51,530 - train - INFO - step:  2  loss:  6.3584  grad_norm:  2.8384\n",
      "2026-02-26 22:12:51,532 - train - INFO - step:  2  loss:  6.3584  grad_norm:  2.8384\n",
      "2026-02-26 22:12:51,579 - train - INFO - step:  3  loss:  5.8352  grad_norm:  2.8503\n",
      "2026-02-26 22:12:51,581 - train - INFO - step:  3  loss:  5.8352  grad_norm:  2.8503\n",
      "2026-02-26 22:12:51,628 - train - INFO - step:  4  loss:  5.2042  grad_norm:  3.1454\n",
      "2026-02-26 22:12:51,630 - train - INFO - step:  4  loss:  5.2042  grad_norm:  3.1454\n",
      "2026-02-26 22:12:51,679 - train - INFO - step:  5  loss:  4.8257  grad_norm:  2.8325\n",
      "2026-02-26 22:12:51,682 - train - INFO - step:  5  loss:  4.8257  grad_norm:  2.8325\n",
      "W0226 22:12:53.140000 81447 .venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:165] Terminating process 82064 via signal SIGTERM\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      ".venv/lib/python3.12/site-packages/torch/jit/_script.py:362: 14 warnings\n",
      "  /workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/jit/_script.py:362: DeprecationWarning: `torch.jit.script_method` is deprecated. Please switch to `torch.compile` or `torch.export`.\n",
      "    warnings.warn(\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test.py::\u001b[1mtest_fsdp_with_itself[float32-bfloat16]\u001b[0m - torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "\u001b[31mFAILED\u001b[0m test.py::\u001b[1mtest_fsdp_with_itself[bfloat16-bfloat16]\u001b[0m - torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "\u001b[31m\u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m2 passed\u001b[0m, \u001b[33m14 warnings\u001b[0m\u001b[31m in 43.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv run python -m pytest \"test.py::test_fsdp_with_itself\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ad77d",
   "metadata": {},
   "source": [
    "**как можно заметить fsdp2 сам против себя не выдает стабильные результаты** так что поменял тесты чтобы они принимали погрешность до 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4a9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                                     [100%]\u001b[0m\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      ".venv/lib/python3.12/site-packages/torch/jit/_script.py:362: 14 warnings\n",
      "  /workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/jit/_script.py:362: DeprecationWarning: `torch.jit.script_method` is deprecated. Please switch to `torch.compile` or `torch.export`.\n",
      "    warnings.warn(\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m\u001b[32m4 passed\u001b[0m, \u001b[33m\u001b[1m14 warnings\u001b[0m\u001b[33m in 42.90s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv run python -m pytest \"test.py::test_fsdp\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run torchrun --standalone --nproc-per-node=2 train.py --fsdp fsdp2 --num-steps-to-profile 3 --snapshots-dir snapshots/fsdp2 --traces-dir traces/fsdp2 --model 1b --seq-len 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480839da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0226 22:24:02.852000 84890 torch/distributed/run.py:852] \n",
      "W0226 22:24:02.852000 84890 torch/distributed/run.py:852] *****************************************\n",
      "W0226 22:24:02.852000 84890 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0226 22:24:02.852000 84890 torch/distributed/run.py:852] *****************************************\n",
      "2026-02-26 22:24:08,673 - __main__ - INFO - model size: 1498482688 parameters.\n",
      "2026-02-26 22:24:08,680 - __main__ - INFO - model size: 1498482688 parameters.\n",
      "2026-02-26 22:24:08,731 - root - INFO - Loading tokenizer from tokenizer.json\n",
      "2026-02-26 22:24:08,733 - root - INFO - Preparing c4_test dataset from ./c4_test\n",
      "2026-02-26 22:24:08,743 - root - INFO - Loading tokenizer from tokenizer.json\n",
      "2026-02-26 22:24:08,745 - root - INFO - Preparing c4_test dataset from ./c4_test\n",
      "/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/profiler/profiler.py:217: UserWarning: Warning: Profiler clears events at the end of each cycle.Only events from the current cycle will be reported.To keep events across cycles, set acc_events=True.\n",
      "  _warn_once(\n",
      "/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/profiler/profiler.py:217: UserWarning: Warning: Profiler clears events at the end of each cycle.Only events from the current cycle will be reported.To keep events across cycles, set acc_events=True.\n",
      "  _warn_once(\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/train.py\", line 279, in <module>\n",
      "[rank0]:     tyro.cli(train)\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/tyro/_cli.py\", line 281, in cli\n",
      "[rank0]:     out = run_with_args_from_cli()\n",
      "[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/train.py\", line 242, in train\n",
      "[rank0]:     pred = model(inputs[\"input\"].to(\"cuda\"))\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torchtitan/models/llama3/model/model.py\", line 576, in forward\n",
      "[rank0]:     h = layer(\n",
      "[rank0]:         ^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1882, in _call_impl\n",
      "[rank0]:     return inner()\n",
      "[rank0]:            ^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1830, in inner\n",
      "[rank0]:     result = forward_call(*args, **kwargs)\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torchtitan/models/llama3/model/model.py\", line 409, in forward\n",
      "[rank0]:     h = x + self.attention(\n",
      "[rank0]:             ^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torchtitan/models/llama3/model/model.py\", line 290, in forward\n",
      "[rank0]:     self.inner_attention(\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torchtitan/models/attention.py\", line 191, in forward\n",
      "[rank0]:     return F.scaled_dot_product_attention(\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.52 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 22.41 GiB memory in use. Of the allocated memory 19.85 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/train.py\", line 279, in <module>\n",
      "[rank1]:     tyro.cli(train)\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/tyro/_cli.py\", line 281, in cli\n",
      "[rank1]:     out = run_with_args_from_cli()\n",
      "[rank1]:           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/train.py\", line 242, in train\n",
      "[rank1]:     pred = model(inputs[\"input\"].to(\"cuda\"))\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "[rank1]:     return forward_call(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torchtitan/models/llama3/model/model.py\", line 576, in forward\n",
      "[rank1]:     h = layer(\n",
      "[rank1]:         ^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1882, in _call_impl\n",
      "[rank1]:     return inner()\n",
      "[rank1]:            ^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1830, in inner\n",
      "[rank1]:     result = forward_call(*args, **kwargs)\n",
      "[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torchtitan/models/llama3/model/model.py\", line 409, in forward\n",
      "[rank1]:     h = x + self.attention(\n",
      "[rank1]:             ^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "[rank1]:     return forward_call(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torchtitan/models/llama3/model/model.py\", line 290, in forward\n",
      "[rank1]:     self.inner_attention(\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "[rank1]:     return forward_call(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torchtitan/models/attention.py\", line 191, in forward\n",
      "[rank1]:     return F.scaled_dot_product_attention(\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 1 has a total capacity of 23.52 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 22.41 GiB memory in use. Of the allocated memory 19.85 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[rank0]:[W226 22:24:10.611994096 unwind.cpp:216] Warning: Unsupported unwinding pattern: unknown op code 0x8 (function unwinderFor)\n",
      "[rank1]:[W226 22:24:10.612909149 unwind.cpp:216] Warning: Unsupported unwinding pattern: unknown op code 0x8 (function unwinderFor)\n",
      "[rank0]:[W226 22:24:10.842583925 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0226 22:24:11.170000 84890 torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 84959 closing signal SIGTERM\n",
      "E0226 22:24:11.337000 84890 torch/distributed/elastic/multiprocessing/api.py:984] failed (exitcode: -11) local_rank: 0 (pid: 84958) of binary: /workspace/efdl_hw/week05_fsdp/homework/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 362, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/distributed/run.py\", line 991, in main\n",
      "    run(args)\n",
      "  File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/distributed/run.py\", line 982, in run\n",
      "    elastic_launch(\n",
      "  File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 170, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/efdl_hw/week05_fsdp/homework/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 317, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "=======================================================\n",
      "train.py FAILED\n",
      "-------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2026-02-26_22:24:11\n",
      "  host      : 517bfe5753aa\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : -11 (pid: 84959)\n",
      "  error_file: <N/A>\n",
      "  traceback : Signal 11 (SIGSEGV) received by PID 84959\n",
      "-------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2026-02-26_22:24:11\n",
      "  host      : 517bfe5753aa\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : -11 (pid: 84958)\n",
      "  error_file: <N/A>\n",
      "  traceback : Signal 11 (SIGSEGV) received by PID 84958\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "!uv run torchrun --standalone --nproc-per-node=2 train.py --fsdp effdl --num-steps-to-profile 3 --snapshots-dir snapshots/effdl --traces-dir traces/effdl  --model 1b --seq-len 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7f2ee",
   "metadata": {},
   "source": [
    "<img src=\"images/image.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6819a9",
   "metadata": {},
   "source": [
    "<img src=\"images/image2.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a5d00",
   "metadata": {},
   "source": [
    "как видно мемори снепшоты получаются очень похожими"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e01da9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
